David Lynch
UTEID: Lynchde
Lecture 34
1. It's impossible to transmit a signal over a channel at an average rate that's greater than C/h because the size of the channel is given as C and the entropy is given as h, so with the perfect encoding, you'd be able to fill up the channel with your encoding and get a rate that's C/h, but you couldn't do any better than that.
2. Increasing the redundancy of the coding scheme can increase the reliability of transmitting a message over a noisy channel  because increasing redundancy gives the message's receiver multiple opportunities to grasp the full message.  Shannon proved that even with a noisy channel, if an encoding was sufficiently redundant, eventually the message could be conveyed.
Lecture 35
1. The entropy of the language is found by calculating the log (base 2) of the number of encodings.  In the case of 10 digits, it's necessary to calculate 10 encodings, and the entropy of the language would work out to about 3.322.
2. The reasons why computing the entropy of a natural language is difficult is because natural language has varied syntax for semantically identical concepts.  The example in class was that "Yup" and "Certainly" convey the same meaning, but require different numbers of symbols.  
3. The difference between the zero, first, second, and third order models is that the zero order model assumes all probabilities of symbols are the same, and each subsequent model (first through third, respectively) calculates the probability of  one additional subsequent adjacent character.  e.g. A second order model would consider the probabilities of every 2 characters, and a third order model every three characters. 
Lecture 36
1. Prior probabilities are sometimes impossible to compute because the semantics of different implementations of an encoding can vary, so the probabilities of one sample might not be indicative of the probabilities of all samples.
2. The reason that the information content of a message is relative to the state of knowledge of the listener is because the probabilities of the next piece of information relies upon the listeners knowledge of the universe of possibilities for the message. 
3. The relationship between entropy and redundancy is that entropy can be used to measure the amount of redundancy in the encoding.  If the number of symbols equals the optimal encoding, then there isn't any redundancy.
Lecture 37
1. An observation that was particularly striking to me during the lecture about the message from Captain Kidd is that a huge amount of the work of decoding a message is based on how successfully the message sender can be "profiled".  In the case of Captain Kidd's message, determining the language and the potential for complexity within the message were both essential to potentially decoding it.
2. A key may be optional for the processes of encryption or decryption because a key can be used as the basis for encrytion and decryption, but it obviously isn't necessary if the encryption is somehow built into the conversion, like the Caeser's Cipher example given on piazza by Prof. Young.
3. The effect of encrypting a file on its information content is hopefully to preserve the content while hiding it.  If the message is lost, than the encryption wasn't really successful.
4. Redundancy in the source can give clues to the decoding process if those regularities persist through to the encryption, and especially if those regularities correspond to regularities in the source language.
Lecture 38
1. If we treat the D() function as the decrytion function and the E() function as the encryption function, and P as plain text, then this is plain text that has been encrypted, and decryted two times, leaving it as just plain text.
2.The decryption function with the key can be paired with the first inner encryption function with the matching key, but there is another encryption function inside of that, so we're going to be left with cipher text from the innermost encryption function.
3. A cryptanalyst would want to recognize patterns in encrypted messages because it might allow them to make accurate inferences about messages without necessarily cracking the algorithim that's used to encrypt the message.
4. Knowing the properties of a language would be useful to a cryptanalyst because it would allow them to identify the commonalities in a language that would be repeated in a message, and those commonalities would give a clue about the method of encoding.  A good example is looking for the most common letter (e) in an english language encoding.
Lecture 39
1. An encryption algorithm might not be feasible to break because there are could be too many possible translations of the code.  Searching through every possible translation might not be an efficient way to communicate.
2. K could be recovered by exhaustive search in an expected time on the order of 2^n-1 operations because that's how long it would take to try every possible transformation key for a bitstring, and most cryptosystems are bit string transformations.
3. Substitution (which is changing a symbol to be another symbol) and transposition (which is changing the ordering of symbols) are both important in ciphers because they are very powerful when combined, and they are simplistic enough to construct a reliable system from them.  
4. The difference between confusion (which is transforming information so an interceptor cannot readily extract it) and diffusion (which is spreading the information widely over the cipertext) is that the former relies primarily on substitution and the latter relies primarily on transposition.  Confusion is aided by (and can be a result of) diffusion. 
5. Confusion and diffusion are both essential for effective encryption, since they are primarily accomplished by substitution and transposition respectively, which are the two main methods for creating effective ciphers.
Lecture 40
1. The difference between a monoalphabetic and polyalphabetic substitution is that with a monoalphabetic ciper, the substitution is the same for each of the substituted letters, and with a polyalphabetic ciper, the substitution changes.
2. The key in a simple substitution cipher is simply a mapping from one alphabet to another alphabet.
3. There are k! mappings from a plaintext to a ciphertext alphabet in simple substitution because a simple substitution is just a mapping, and the maximum number of possible combinations for the mapping is k!.
4. The key in the Caesar Cipher example is the size of the shift in the alphabet, which is a number of characters that it's necessary to move to give you the new character.
5. The size of the keyspace in the Caesar Cipher is either 25 or 26 letters, but a shift of 26 letters would just mean that it was accessing the same letter that it had translated from the original text.
6. In the lecture, Prof. Young says that the Caesar Cipher algorithm isn't strong because you probably don't have to try too many options before you find a match, and you probably won't have to try all the options within the universe of options.
7.  The corresponding decryption algorithm to the Vigenere ciphertext is to look up the corresponding letter in the key, and then scan across the row (or column) to find the letter that's used in the cipher.  The column (or row, respectively) that aligns with the letter is the original character.
Lecture 41
1. There are 17576 possible decryptions for the "xyy" encoding on slide 3 because, if we are assuming that each letter could correspond to a single other letter in the decryption, that leaves us with 26^3 options, and 26^3 is 17576.
2. The search space for question 2 on slide 3 is reduced be a factor of 27 because we find out that it's a simple substitution cipher, and there's repetition in the code.
3. I think a perfect cipher could be possible if the encrypted message was small and the key was extensive.  For instance, it a different key was used for every message and there wasn't any repetition, and the messages were encoded numerically each time, I could conceive of a situation where a cryptanalyst was unable to reduce the search space, but it wouldn't really be practical.  In the following lecture, the possibility of a perfect cipher was confirmed with the concept of a one-time pad. 
Lecture 42
1. The reason that one-time pad offers perfect encryption is because, even when given the encryption algorithm, the keys are different each time, so decrypting the original text is impossible.
2. It's important that the key in a one-time pad is random because if it wasn't, then there would be repetition in the XOR'd string related to the repetition of the key string that would give the cryptanalyst an insight to narrow the search space for each string that's been XOR'd.
3. The key distribution problem is that securely distributing the keys is as difficult as securely distributing the message, and any secure distribution of a key suffers from the same vulnerabilities as the secure distribution of the message. 
Lecture 43
1. The downside to using encryption by transpostion is that it doesn't change anything about the frequency of the characters in the text, and that repetition can give insight into the cipher.
Lecture 44
1. A one-time pad is a symmetric algorithm because, because it has to use the same key for encrypting and decrypting.  Since it could be viewed as a bitstring that's XOR'd with another bitstring, if it's XOR'd with a different bitstring, it won't arrive at the same initial message.
2. The difference between key distribution and key management is that key distribution just involves the safe transmission of a key, and key management is making sure that the keys that are being used continue to be functional with increased distribution.
3. If someone got a hold of the secret key in a public key system, they could decrypt S's encrypted message in the same way that the intermediary decrypts the message when it has the secret key. 
4. They both have drawbacks and benefits that arise based on their implementation.  Symmetric bitstrings are simple to generate, but there need to be a different one for every participant in the system, which is a quadratic number of keys, but they're much more difficult to guess.  In a public key system, the keys need to have a special structure if they're going to be viable, and that makes them expensive to generate, but there can be a linear number of keys in the system which are much easier to guess than a random bitstring because they need to fit the keystring structure.
Lecture 45
1. Most modern symmetric encryption algorithms are block ciphers because most ciphers aren't simple substitution ciphers, so they need the context of the surrounding characters.
2. The significance of malleability is that it implies that you can make changes to the cipher text in ways that will have predicatble effects on the plaintext.  It's seen as a very bad thing.
3. The significance of homomorphic encryption is that if the same changes are being applied to the same characters each time, the cipher is going to be susceptible to an anaylst that looks for repetition.
Lecture 46
1. The first step in AES (subBytes) is used in confusion, and it's accomplished by using each byte as an index into a 256 element lookup table.
2. The second step in AES (shiftRows) is used in diffusion, and it's accomplished by zero indexing the rows and then shifting each by their row number.
3. Decryption takes longer than encryption in AES because it's much more difficult to optimize the array multiplication with the larger numbers in the array that's necessary to get back to the original data.
4. Blocks are the units that data is stored in to be processed by AES, and rounds are used in AES as iterations through the algorithm. 
5. Someone might want to increase the total number of Rounds in AES because (presumably) each round would require additional calculations to decrypt, and so the additional rounds could dissuade potential attackers.
Lecture 47
1. The disadvantage in using ECB mode is that the same blocks of data will have the same cipher blocks, so it's susceptible to analysts who are looking for repetition.
2. That disadvantage can be fixed by a process like cipher block chaining, which randomizes the data's original bocks.  The goals of any process is going to be to randomize the original data.
3. A potential weakness of CBC (cipher block chaining) is that there can be identical blocks at the beginning of two messages, and when those messages begin to differ, an astute analyst will be able to tell that CBC has begun.  Also, XOR two blocks of identical cipher text together will give information about the underlying plaintext. 
4. Key stream generation is different than standard block encryption because the cipher is used to generate numbers that are used as a one time pad, and then that pad is used in decryption.
Lecture 48
1. For public key systems, the private key must be kept secret in order to ensure secrecy.
2. One way functions are critical to public key systems because if the function is as easy to undo as it is to compute, then the encryption wouldn't be secure.
3. Public key systems solve the key distribution problem by making sure that anyone can encrypt with a key, but only the eventual reciever will ever be able be decrypt it.
4. This is a little odd.  I think it shows that the private key is used first, followed by the public key, followed by the private key on that transformed block of data, but I think this isn't going to yield a result that makes sense.  I think it's going to be an encrypted block of text that's gotten more encrypted without ever being solved.
5. Asymmetric algorithms are more efficient than symmetric algorithms because they don't have to rely on computationally intensive one-way functions to encrypt, and that's why they're not used so often in commercial applications.
Lecture 49
1. If one generated new RSA keys and switched the public and private keys, the algorithm would still work because they function in a way where the public key can undo the work of the private key and vice-versa.
2. In RSA, prime numbers are used to factor keys and decrypt messages with those factorizations.
3. RSA is breakable because the transformation of the message can eventually be factored by using large prime numbers, but it can't be efficiently factored.
4. The reason why an intercepted message can't be read with RSA is that the person who is intercepting the message doesn't have the private key that can be used to factor the encryption of the message.
5. A can't be sure that {M}Ka came from B because the public key isn't guarded, so anyone can send an encrypted message using the public key.
6. The holder of the public key can be sure that a message was initiated by the holder of the private key because the holder of the private key is unique.
7. Someone intercepting a message who is holding the public key can read it the same way that anyone else holding a public key can.  They know it's from the private key, and as long as they have a public key it's open to them, so there's no confidentiality.
8. B can ensure authentication and confidentiality when sending a message to A by having two sets of keys, one for determining authenticity and one for encryption.
Lecture 50
1. Hash functions must be easy to compute for any given data because it's introducing a layer onto that data, and moving through that layer to access the data needs to be efficient or people will choose to not bother with hashing.
2. The key difference between strong and weak collision resistance of a hash function is that with weak collision resistance, it's difficult to find two values that don't equal each other but still hash to the same value, but with strong collision resistance, it's difficult to find any 2 values that hash to the same value. 
3. Preimage resistance means that if we're given a specific hash value, it's difficult to find an initial value that hashes to that value, and second preimage resistance is a metric that says that as long as 2 initial values don't equal each other it's difficult to find 2 hash values that collide.
4. A birthday attack on a 128 bit hash value implies that you'll have to look at 1.25x(2^64) values before you find a collision. 
5. A birthday attack on a 160 bit hash value implies that you'll have to look at 1.25x(2^80) values before you find a collision. 
6. Cryptographic hash functions aren't used for confidentiality because all they do is provide a condensed representation of data, which doesn't provide any benefit in terms of determining confidentiality.  They are orthogonal issues and this mechanism isn't useful for confirming that a chunk of data hasn't been accessed in an insecure fashion.
7. The attribute of a hash function that ensures that a message is tamper resistant is that it's collision resistant.
8. B could securely send a message to A and guarantee both confidentiality and integrity by sending the results of the hashed message in the RSA text.  Once the message is recieved, it can be rehashed using the same hash function, and if the results of the two hashings are the same, then its integrity and confidentiality are ensured.
Lecture 51
1. S can't sent the key because the notation is indicating that they are both going to be using private keys, but the entire basis of assymetric encryption is that there's only one private key between a sender and a reciever for a message.  The reciever's private key can't be accessed by the sender.
2. S couldn't have done the encryptions in the other order because that would mean that it was taking the key and first applying the reciever's public key, and then applying the sender's private key, and this would mean that the reciever would have to decrypt with the sender's private key, and then their own public key, but that's not feasible because they won't be able to decrypt the outer layer of the message to get to the inner layer.
3. The first notation is equivalent to the second notation because the public key of the sender is available to the reciever and the sender, so including the public key of the sender is something that the reciever would have already given to the sender.
4. The requirements of key exchange are confidentiality and authentication, because without either of these two components, a key exchange could be insecure.  For example, if the key exchange wasn't confidential, then anyone could have accessed the key, and if the key exchange wasn't authenticated then a black hat could've intercepted and be sending erronous messages with the key.
Lecture 52
1. If an eavesdropper was listening in on a diffie-hellman exchange and they knew g, p, and g^a mod p they still wouldn't have the agreed upon key, which is the entire point of the exchange.
2. If a was discovered by an eavesdropper listening in on a Diffie-Hellman exchange then they would be able to walk back through the exchange to find the key, but they would have to be familiar with the protocols, and also have all the other data that was listed in question 1.  Also, this is a number that's chosen by the individual, so that number would never be explicitly released in any communication.
3. This answer is very similar to the answer above.  If b was discovered by an eavesdropper listening in on a Diffie-Hellman exchange then they would be able to walk back through the exchange to find the key, but they would have to be familiar with the protocols, and also have all the other data that was listed in question 1.  Once again, this is a number that's chosen by the individual B, so that number would never be explicitly released in any communication.
