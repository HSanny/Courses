David Lynch
UTEID: Lynchde
Lecture 66
1. PGP is an acronym that stands for "pretty good privacy" and it incorporates the best quality cryptographic algorithms that are available online.
2. Phil Zimmerman was motivated to develop PGP because he wanted to allow the average computer user to use the high quality cryptographic algorythims that were available to advanced computer science users and integrated these into a general purpose algorithm that was easy for anyone to use.  His libertarian outlook colored his motivations.
3. PGP provides effective security because it incorporates many of the best cryptographic algorithms and methods.
4. People would purchase support for PGP because it gives them a party that they can hold accountable if the software doesn't perform exactly as it's intended to perform.
Lecture 67
1. The PGP authentication protocol involves sending the message with the message senders private key, which ensures that they are the only ones who could've sent that message.
2. The PGP confidentiality protocol is acccomplished by encrypting the session key with the receiver's public key, so nobody except the holder of the private key can access the message.
3. You get both authentication and confidentiality when the parts that I described in question 1 and 2 are combined, since they achieve authentication and confidentiality.
Lecture 68
1. Besides authentication and confidentiality, PGP provides other services like compression, email compatibility and segmentation.
2. Compression is needed because it allows messages to be efficiently sent over the internet.
3. The reason why an algorithm would sign a message and then compress (as opposed to compressing and then signing a message)is that then the large signature would be compressed.  I imagine this would be useful for private keys that are particularly long, and it prevents the signature from being dependent on the compression algorithm.
4. Radix-64 conversion is a substition portion of PGP that maps groups of 3 octets into four ASCII characters, and it's needed because it eliminates the possibility that an email message might contain arbitrary sequences that an email program interprets as commands.
5. PGP segmentation is needed because some messages are too long for certain types of email providers, and segmentation allows long messages to be broken up and reassembled at the email's destination.
Lecture 69
1. The four kinds of keys used by PGP are session keys, public keys, private keys, and passphrase-based keys.
2. The special properties needed of session keys are that they are high entropy so they won't be vulnerable to being guessed by an attacker.
3. Session keys are generated by accumulating entropy by collecting keystrokes and mouse moves from the user. 
4. Since RSA is based on prime numbers keys are generated by generating keys that are of a certain length, and testing the key for primality.   
5. Private keys are protected by a passphrase-based key, and this is necessary because otherwise, access to a private key only requires access to the disk on which it's stored.
Lecture 70
1. A user knows which private/public key pair is used when he receives an encrypted message by checking an id which is composed of the last 64 bits of the public key and it's attached to the message. 
2. A user's private key ring contains a timestamp, a key ID, the public key, the private key (which is encrypted with a passphrase-based key, and the user id, which is usually just their email address.
3. A user's public key ring contains a timestamp, key id, public key and the user id as described above. 
4. The steps in retrieving a private key from the key ring are to enter a passphrase (which is hashed),  and then recover the session key.
5. The key legitimacy field is to allow you to determine a level of trust for a public key and determine that it was the one that was given to a person.  It functions much like a certificate.
6. A key is revoked by sending a revocation certificate which can't force someone to stop using that key, but functions as an advisory to let people know that you'd prefer it if they didn't use that key anymore.
Lecture 71
1. The difference between the consumer and producer problems is that the consumer problem is when an attacker injects itself between the client and the server, and the producer problem is when the attacker goes directly after the server.  The more prevalent of the two is the producer problem.
2. Syn flooding is where a protocol is initiated and the server's resources (the server's table) are occupied with waiting for a response while the attacker refuses to respond.  It relies on the TCP three way handshake.
3. The first three solutions to syn flooding aren't ideal because respectively, increasing the queue size leaves a server vulnerable to attacks that fill up the larger queue, shortening the time out period will disallow legitimate connections by slower clients which basically is the denial of service attack that the solution was seeking to prevent, and filtering suspicious packets is flawed since it's difficult to determine what packets are actually suspicious.
Lecture 72
1. Packet filtering works well to prevent attacks because you can set the filter to a level that eliminates all but the most trusted packets.  Unfortunately, this also serves to eliminate legitimate traffic as well.
2. The differences between intrusion detection and intrusion prevention systems is that an intrusion detection system tends to respond after an attack has already begun, and and intrusion prevention system tries to stop attackers before an attack can begin.
3. The four different solutions mentioned to DDoS attacks are to over-provision the network by purchasing servers to raise a system's resources, filtering attack packets, slow down processing to adversely impact the attacker, and to request additional traffic from a malicious attacker with the understanding that attackers probably have already maximized their capabilities.
Lecture 73
1. A false positive is when a genuine attack takes place that you don't detect and a false negative is when benign behavior is interpreted as an attack.  Determining which is worse depends on the context.  False positives could be worse if, for example, your system is losing important and confidential information, and false negatives could be worse if your system is reliant on users who are sensitive to a website's downtime.
2. In the IDS context, accurate means that all genuine attacks are detected and precise means that it never reports legitimate behavior as an attack.
3. The reason that it's easy to build an IDS that's either accurate or precise is because, if we reduce this problem to absurdity, we can see that as long as we detect every interaction as an attack, it will be entirely accurate, and as long as we report no attacks, it means it will be perfectly precise.
4. The base rate fallacy is when you have fairly high accuracy, but you still have a large number of false positives, and it's relevant to an IDS because a seemingly high rate of detection accuracy (like 90%) can still lead to a high number of false alarms (92% of all alarms in the case of a 90% accuracy rate).
Lecture 74
1. Code Red version 1 attempted to break into a system and used the accessed system to launch a denial of service attack on the whitehouse website.
2. It was ineffective for lots of reasons, but one of the primary ones was that the random IP address generator wasn't entirely random so it wasn't difficult to change the IP addresses of the target and figure out what IP addresses would be accessed next so it spread very slowly.
3. When we say that a worm is memory resident, we're saying that it implies that rebooting the machine and wiping the memory will also eliminate the worm.
4. Code Red version 2 was more effective than version 1 because it used a truely random number generator to choose the IP addresses to infect next.
Lecture 75
1. Code Red II was related to Code Red because the writer was clearly aware of the first Code Red, because it came out a few weeks later and it used the word "Code Red" in the code.
2. Code Red II's elaborate propogation scheme was a correction of the flaw of the first worm, which was memory resident, so it could be eliminated with a simple reboot.
3. Code Red II attempted to create a botnet, presumably with the intention of eventually launching a denial of service attack, but it had a much more elaborate scheme for ensuring the reliability of the machines it infected and a better way for attempting to choose the IP addresses that it would attempt to infect than the machines that Code Red I infected.
4. The implications of a large population of unpatched machines in the face of something like Code Red II is that the worm might be able to persist far longer than anticipated, and that it could potentially persist and leave residual vulnerabilities long after security professionals have identified and focused on other concerns.
5. The lessons that we can derive from the Verizon study are that keeping security software updated will prevent the majority of attacks.
Lecture 76
1. A certification regime for secure products is necessary and useful because it helps consumers determine the type of security product that fits their needs.
2. The components of an evaluation standard are a set of requirements defining security functionality, a set of assurance requirements for establishing the functionality requirements, a methodology for determining that the functional requirements have been met, and a grading or evaluation system.
3. Crypto devices would require a separate evaluation mechanism from certification regimes because they are going to be used for wildly different things than, forinstance, a firewall, so their differing functionalities demand differing methods for evaluating their efficacy.
4. The four levels of certification for crypto devices are are defined in FIPS 140-2, and are numerically ordered 1-4, and they describe, basic security, improved physical security, increased tamper-resistance, and the last level is a complete protection envelope.
Lecture 77
1. The common criteris is an evaluation standard for every non-cryptographic device.
2. The word common is used because the evaluation standard isn't related to any given nationality.
3. The reason why there would be a need for a national scheme is that some nations might attempt to embed features within software that would make systems insecure with the intent of taking advantage of those vulnerabilities for percieved national interests.
4. The difference between a protection profile and a security target is that a protection profile defines the characteristics that certain security products should contain, and the security target is the actual system that's supposed to fit within the definition of the protection profile.
Lecture 78
1. The overall goal of the protection profile as exemplified by the WBIS example is to identify the various threats and assumptions and goals of the device, and how the security requirements can be used to accomplish the security objectives.
2. The purpose of the various parts of the protection profile are security objectives, security requirements, threats, assumptions.
3. The matrix on slide 7 shows that all the threats have a systematic and validated way of dealing with them.
Lecture 79
1. The overall goal of the security target evaluation in the Sun Identity Manager example is to certify that the system had had successfully implemented measures that allowed them to achieve security that was equivalent to the specifications for EAL2.  
2. A security target evaluation differs from a protection profile evaluation because it's dealing with a specific product instead of a broader general type of product.  It has more to do with providing proof that goals were sucessfully met than establishing what reasonable levels of functional goals should be.
Lecture 80
1. EALS (Evaluation assurance levels) are listed in numerical order from 1 to 7, and they're used for providing indications of the type of evidence that would be required to be gathered to prove that a product is as secure as the specification for the policy requested. 
2. The common criteria evalustions are certified by the government of the country where the evaluation is performed, and so at the higher security levels, NSA employees are used to perform the evaluations, but at lower levels, they're completed by independent labs.
3. The higher EALs aren't mutually recognized by various countries because the higher levels are based on specific tests that are country-sensitive, and other countries wouldn't want to leave themselves vulnerable to other countries evaluation of their systems, especially when another country could certify an insecurity that would leave a countries system open to vulnerabilities.  It's based on mutual suspicion.
4. Vendors can't certify their own products because then everyone would be certified EAL 7.  There would be an incentive to claim that products were of the highest security level, and no objective outside party to verify it.  
5. Reverse engineering the model from the code would be bad for a formal evaluation because it would create an approach to building a secure product that's based off a specification that can't necessarily be comprehensive.  So instead of seeking a truly secure product, it would be seeking a solution to a test.  