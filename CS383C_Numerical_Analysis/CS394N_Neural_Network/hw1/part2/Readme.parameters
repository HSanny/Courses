Description of the parameters in the simulation file (e.g. simu):

---------------------------------------------------------------------------
## Name of the file containing the vectors to be mapped. This file has:
##  (1) the name of the file containing the labels for the vectors, e.g "voc"
##  (2) the dimensionality of the vectors, e.g. "12"
##  (3) the components of the vectors, concatenated. Every vector must have
##      a label in the labelfile
vectorfile		;;; vectorfile

## Dimensions of the feature map network; here it is a square array of
##   10 by 10 units. A good rule of thumb is that you should have about
##   four times as many units as you have vectors.
10 10 			;;; nx, ny

## disp_rate: 0=no display (e.g. when training in the background).
## Other integers specify how often to display, e.g. 1 (always) 
## or 3 (every third iteration.)
##   The display is alphanumeric.
## seed: random number generator seed (no need to change it).
1 1 			;;; disp_rate, seed

## The next few lines contain the parameters you have to tune to get
##   the map to self-organize properly: 
## tend: number of training presentations (time-end).
## nphase: number of training phases (described below).
## nc_[-1]: initial neighborhood radius. Units whose x and y coordinates are
##  within this distance of the winning unit are modified.
##  Initially the neighborhood should cover almost the whole network, and 
##  then gradually decrease to 0.
## alpha_[-1]: initial learning rate (at time 0). Should also gradually
##  decrease to 0.
15000 3 4 0.1		;;; tend, nphase, nc_[-1], alpha_[-1]

## Parameters describing each training phase:
## t_[phase]: end time of the phase.
## nc_[phase]: neighborhood radius at the end of the phase. During the phase,
##   the neighborhood is decreased from nc_[phase-1] to nc_[phase]
##   in discrete steps approximating linear decrease. It reaches the value
##   nc_[phase] at time t_[phase].
## alpha_[phase]: learning rate at the end of the phase. Also decreases
##   linearly during the phase.
1000 1 0.05		;;; t_[phase], nc_[phase],alpha_[phase]
8000 0 0.025		;;; t_[phase], nc_[phase],alpha_[phase]
15000 0 0.0		;;; t_[phase], nc_[phase],alpha_[phase]

## Times when snapshots are saved. Below, only one snapshot at the end
##   of the simulation at t=15000 is saved. 999999999 is just a sentinel
##   ending the list of snapshot times. E.g., if you want to save a
##   snapshot at the beginning, and at times 100 500 and 1000, you would say
##   -1 100 500 1000 999999999
15000 999999999 	;;; snapshots (epoch):0=initial, 9x9=last

## snapshots are stored here, below the list of snapshot times...
---------------------------------------------------------------------------

For example, if you want to first train the net for 500 iterations in
the background, check how it is doing, and then continue until t=1000,
this is what you would do:

1. Specify tend=500, disp_rate=0, snapshots = -1 100 500 1000
2. nohup nice sofm simu &
   ( the network is trained for 500 presentations, and the state of the
     net is stored in the beginning, at t=100 and in the end. )
3. Change disp_rate=1
4. sofm simu
   ( now each snapshots is displayed on the screen. Hit return to get
     the next snapshot. )
5. Change tend=1000, disp_rate=0
6. nohup nice sofm simu &
   ( the simulation is continued from the last stored snapshot, and run
     until tend )
7. Do 3. and 4. again to view the results.

Of course, you can train the network on screen (with disp_rate != 0),
and that is probably a good thing to do when you first try to get a
feel of the self-organizing process. Later, with larger networks, the
training takes longer and you want to turn the display off and only
look at the snapshots.
