%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  THIS TEX FILE IS TO GENERATE PDF FILE FOR 
%%% 
%%%  COPYRIGHT (C) JIMMY LIN, 2013, UT AUSTIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,a4paper]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  PACKAGES USED IN THIS TEX SOURCE FILE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry,amsthm,amsmath,graphicx,fancyheadings,amsfonts,tikz}
\usepackage{../mcode}
\usepackage[colorlinks,
            linkcolor=blue,
            anchorcolor=red,
            citecolor=green
            ]{hyperref}
% for my mac
\IfFileExists{/Users/JimmyLin/.latex/UTA_CS/JS.sty}{ 
    \usepackage{/Users/JimmyLin/.latex/UTA_CS/JS}
    \usepackage{/Users/JimmyLin/.latex/UTA_CS/JSASGN}
}{} 
% for UT's linux machine
\IfFileExists{/u/jimmylin/workspace/Configs/latex/UTA_CS/JS.sty}{
    \usepackage{/u/jimmylin/.latex/UTA_CS/JS} 
    \usepackage{/u/jimmylin/.latex/UTA_CS/JSASGN}
}{} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% MACROS CONTAINING THE FILE INFORMATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\COURSE}{CS383C Numerical Analysis}
\renewcommand{\LECTURER}{Robert A. van de Geijn}
\renewcommand{\SECTION}{53180}
\renewcommand{\TASK}{Midterm Exam}
\renewcommand{\RELEASEDATE}{Oct. 16 2014}
\renewcommand{\DUEDATE}{Oct. 21 2014}
\renewcommand{\TIMECONSUME}{10 hours}

\renewcommand{\thesection}{Exercise \arabic{section}.}
\renewcommand{\thesubsection}{(\alph{subsection})}
\renewcommand{\contentsname}{Exercises}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% DOCUMENTATION STARTS FROM HERE 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE PAGE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
    \maketitle
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CONTENT PAGE: TABLEOFCONTENTS, LISTOFTABLES, LIST OF FIGURES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center} 
    \tableofcontents  
%
%    %\listoftables 
%    %\listoffigures
\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% GENERAL DOCUMENTATION BEGINS 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{}
\subsection{Prove that $||\cdot||$ is a vector norm.}
To prove $||\cdot||$ is a vector norm, we have to show all the following three
property holds: 
\begin{itemize}
    \item If $x\not = 0$, then $|| x || > 0$.
    \item For arbitrary scalar $\alpha$ and vector $x$, $||\alpha x|| = | \alpha | \cdot || x || $.
    \item For arbitrary vector $x$ and $y$, $|| x + y || \leq || x || + || y ||$.
\end{itemize}
Now we turn to prove each of them individually.
\begin{proof}
    Since $D$ is diagonal with positive diagonal elements, then $\forall i,\
    \delta_i > 0$ holds. Besides, since $x \not = 0$, then $\exists i,\ x_i^2 > 0$.
    \begin{align}
        || x || = \sqrt{x^T D x} &= \sqrt{\sum_{i=0}^{m-1} \delta_i x_i^2} > 0
        && x \not = 0
    \end{align}
    Therefore, we proved that if $x\not = 0$, then $|| x || > 0$.
\end{proof}
\begin{proof}
    \begin{align}
        || \alpha x || = \sqrt{(\alpha x)^T D (\alpha x)} 
        = \sqrt{\sum_{i=0}^{m-1} \alpha^2 \delta_i x_i^2} 
        = \sqrt{\alpha^2\sum_{i=0}^{m-1}  \delta_i x_i^2} 
        = |\alpha| \sqrt{\sum_{i=0}^{m-1}  \delta_i x_i^2} 
        = |\alpha| \cdot || x ||
    \end{align}
\end{proof}
\begin{proof}
    \begin{align}
        || x + y ||^2 &= (\sqrt{(x+y)^T D (x+y)})^2 
        = x^T D x + x^T D y + 2x^T D y  \\
        (|| x || + || y ||)^2 &= (\sqrt{x^T D x})^2 + (\sqrt{y^T D y})^2
        = x^T D x + y^T D y + 2\sqrt{x^T D x}\sqrt{y^T D y} 
    \end{align}
    Now we show that $ x^T D y \leq \sqrt{x^T D x}\sqrt{y^T D y}$, which is
    equivalent to
    \begin{align}
        & \big( \sum_{i=0}^{m-1} \delta_i x_i y_i \big)^2 \leq 
        \big(\sum_{i=0}^{m-1} \delta_i x_i^2 \big) \big( \sum_{i=0}^{m-1}
        \delta_i y_i^2 \big) \\
        \Longleftrightarrow \hspace{0.5cm}
        &  \sum_{i=0}^{m-1} \sum_{j=0}^{m-1} \delta_i \delta_j x_i y_i x_j y_j \leq
        \sum_{i=0}^{m-1} \sum_{j=0}^{m-1} \delta_i \delta_j x_i^2 y_j^2  \\
        \Longleftrightarrow \hspace{0.5cm}
        &  \sum_{i=0}^{m-1} \sum_{j=0}^{i-1} \delta_i \delta_j ( 2 x_i y_i x_j y_j)
        + \sum_{i=0}^{m-1} \delta_i^2 x_i y_i x_i y_i
        \leq
        \sum_{i=0}^{m-1} \sum_{j=0}^{i-1} \delta_i \delta_j (x_i^2 y_j^2 + x_j^2 y_i^2)
        + \sum_{i=0}^{m-1} \delta_i^2 x_i^2 y_i^2 \\
        \Longleftrightarrow \hspace{0.5cm}
        &  \sum_{i=0}^{m-1} \sum_{j=0}^{i-1} \delta_i \delta_j ( 2 x_i y_i x_j y_j)
        \leq
        \sum_{i=0}^{m-1} \sum_{j=0}^{i-1} \delta_i \delta_j (x_i^2 y_j^2 + x_j^2 y_i^2)
    \end{align}
    which obivously holds if we apply basic inequality on each $(i,j)$ pair, 
    \begin{align}
        2 x_i y_i x_j y_j = 2 (x_i y_j) (x_j y_i) \leq x_i^2 y_j^2 + x_j^2 y_i^2
    \end{align}
    Since we have proved $ x^T D y \leq \sqrt{x^T D x}\sqrt{y^T D y}$, then
    \begin{align}
        || x + y ||^2 \leq (|| x || + || y ||)^2
    \end{align}
    which leads to desired triangle inequality since $|| \cdot || \geq 0$,
    \begin{align}
        || x + y || \leq || x || + || y ||
    \end{align}
\end{proof}
Since all three property have been proved, it can be concluded that 
\begin{align}
    || \cdot || \text{ defined by $|| x || = \sqrt{x^T D x} $ is a norm. }
\end{align}

\newpage
\subsection{Show that $|| \cdot ||$ is a matrix norm.}
To prove $||\cdot||$ is a matrix norm, we have to show all the following three
property holds: 
\begin{itemize}
    \item If $A \not = 0$, then $|| A || > 0$.
    \item For arbitrary scalar $\alpha$ and matrix $A$, $||\alpha A|| = | \alpha | \cdot || A || $.
    \item For arbitrary matrix $A$ and $B$, $|| A+B || \leq || A || + || B ||$.
\end{itemize}
Now we turn to prove each of them individually.
\begin{proof}
    Since $D$ is diagonal with positive diagonal elements, then $\forall i,\
    \delta_i > 0$ holds. And Since $A \not = 0$, then there is at least one
    column vector that is not zero vector. Let us assume it is the $j$-th
    column, $a_j$, that is non-zero. Then
    \begin{align}
        || A || = || DA ||_2 
        &= \max_{||x||_2 \not=0} \frac{||DAx||_2}{||x||_2}
        \geq \frac{||DAe_j||_2}{|| e_j ||_2} > 0
    \end{align}
    Therefore, we proved that if $A \not = 0$, then $|| A || > 0$.
\end{proof}
\begin{proof}
    \begin{align}
        || \alpha A ||  
        &= || D (\alpha A) ||_2 \\
        &= ||\alpha  D A ||_2 \\
        &= |\alpha| \cdot || D A ||_2 && ||\cdot||_2 \text{ is a norm. } \\
        &= |\alpha| \cdot || A ||
    \end{align}
    Thus, we proved that $|| \alpha A || = |\alpha| \cdot || A ||$.
\end{proof}
\begin{proof}
    \begin{align}     
        || A + B || &= || D (A + B) ||_2  \\
        &= \max_{x\not=0} \frac{||DAx+DBx||_2}{||x||_2} \\
        &\leq \max_{x\not=0} \frac{||DAx||_2+||DBx||_2}{||x||_2} \\
        &= \max_{x\not=0} \bigg(\frac{||DAx||_2}{||x||_2} +
        \frac{||DBx||_2}{||x||_2} \bigg) \\
        &\leq \max_{x\not=0} \frac{||DAx||_2}{||x||_2} +
        \max_{y\not=0} \frac{||DBy||_2}{||y||_2}  \\
        &= || DA ||_2 + || DB ||_2 = || A || + || B ||
    \end{align}
    Hence, we proved that $|| A + B || \leq || A || + || B ||$.
\end{proof}
Since all three property have been proved, it can be concluded that 
\begin{align}
    || \cdot || \text{ defined by $|| A || = || DA ||_2 $ is a norm. }
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{}
\subsection{Show that $(..)^{-1} = (..)$.}
\begin{proof}
\begin{align}
    \left( \begin{array}{c|c} 
        L_{00} & 0 \\ \hline
        l_{10}^T & \lambda_{11}
    \end{array} \right)
    \left( \begin{array}{c|c} 
        L_{00}^{-1} & 0 \\ \hline
        -\frac{l_{10}^T L_{00}^{-1}}{\lambda_{11}} & \frac{1}{\lambda_{11}}
    \end{array} \right)
    = 
    \left( \begin{array}{c|c} 
        L_{00} L_{00}^{-1}  & 0 \\ \hline
        l_{10}^T L_{00}^{-1}-\frac{l_{10}^T L_{00}^{-1}}{\lambda_{11}}\cdot \lambda{11} 
        & \frac{1}{\lambda_{11}} \cdot \lambda_{11}
    \end{array} \right)
    =\left( \begin{array}{c|c} 
        I & 0 \\ \hline
        0 & 1
    \end{array} \right)
    = I
\end{align}
Hence, it is concluded that (assume all inversion exists)
\begin{align}
\left( \begin{array}{c|c} 
        L_{00} & 0 \\ \hline
        l_{10}^T & \lambda_{11}
    \end{array} \right)^{-1}
= \left( \begin{array}{c|c} 
        L_{00}^{-1} & 0 \\ \hline
        -\frac{l_{10}^T L_{00}^{-1}}{\lambda_{11}} & \frac{1}{\lambda_{11}}
    \end{array} \right)
\end{align}
\end{proof}
\subsection{Show that $H_1 H_0 = ..$}
According to given formula for $H_i$, we have
\begin{align}
    H_0 = I - \frac{1}{\tau_0} u_0 u_0^T \\
    H_1 = I - \frac{1}{\tau_1} u_1 u_1^T
\end{align}
Now we manipulate $H_1 H_0$,
\begin{align}
    H_1 H_0 
    &= (I - \frac{1}{\tau_1} u_1 u_1^T) (I - \frac{1}{\tau_0} u_0 u_0^T) \\
    &= I - \frac{1}{\tau_1} u_1 u_1^T - \frac{1}{\tau_0} u_0 u_0^T  +
     \frac{1}{\tau_0\tau_1} u_1u_1^T u_0u_0^T \\
     &= I - (u_0 | u_1) 
     \left( \begin{array}{c|c} 
             \frac{1}{\tau_0} & 0 \\ \hline
             - \frac{u_0^T u_1}{\tau_0\tau_1} & \frac{1}{\tau_1}
         \end{array} \right)
     (u_0 | u_1)^T \\
     &= I - (u_0 | u_1) 
     \left( \begin{array}{c|c} 
             \tau_0 & 0 \\ \hline
              u_0^T u_1 & \tau_1
          \end{array} \right)^{-1}
     (u_0 | u_1)^T
\end{align}
Hence, it is concluded that (assume all inversion exists)
\begin{align}
    H_1 H_0 &= I - (u_0 | u_1) 
     \left( \begin{array}{c|c} 
             \tau_0 & 0 \\ \hline
              u_0^T u_1 & \tau_1
          \end{array} \right)^{-1}
     (u_0 | u_1)^T
\end{align}

\subsection{Show that $(1-\frac{1}{\tau_1 u_1 u_1^T}) .. = I - ...$}
\begin{align}
    (I - \frac{1}{\tau_1} u_1 u_1^T) (I - U_0 L_{00}^T U_0^T) 
   &= I - \frac{1}{\tau_1} u_1 u_1^T - U_0 L_{00}^T U_0^T
   + \frac{1}{\tau_1} u_1 u_1^T U_0 L_{00}^T U_0^T \\
   &= I - (U_0 | u_1) 
      \left( \begin{array}{c|c} 
              L_{00}^{-1} & 0 \\ \hline
              - u_1^T U_0 L_{00}^{-1}/\tau_1 & \frac{1}{\tau_1}
          \end{array} \right)
   (U_0 | u_1)^{T}  \\
   &= I - (U_0 | u_1) 
      \left( \begin{array}{c|c} 
              L_{00} & 0 \\ \hline
               u_1^T U_0 & \tau_1
           \end{array} \right)^{-1}
   (U_0 | u_1)^{T} 
\end{align}
Thus, it is concluded that (assume all inversion exists)
\begin{align}
    (I - \frac{1}{\tau_1} u_1 u_1^T) (I - U_0 L_{00}^T U_0^T) 
  &= I - (U_0 | u_1) 
      \left( \begin{array}{c|c} 
              L_{00} & 0 \\ \hline
               u_1^T U_0 & \tau_1
           \end{array} \right)^{-1}
   (U_0 | u_1)^{T} 
\end{align}

\newpage
\subsection{Show that $H_{k-1} \cdots H_1 H_0 = I - UL^{-1} U^H$}
We can do mathematical induction on $m$. We separate the discussion of $k = 1$
and $k > 1$. 
\begin{itemize}
    \item Particular Case: $k=1$, 
        \begin{proof}
        \begin{align}
            H_0 = I - \frac{1}{\tau_0} u_0 u_0^T 
            = I - \underbrace{(u_0)}_{U_{(0)}} 
        \underbrace{(\frac{1}{\tau_0})}_{L^{-1}_{(0)}} 
        \underbrace{(u_0)^T}_{U_{(0)}^T}
        \end{align}
        Hence, $H_{k-1} \cdots H_1 H_0 = I - UL^{-1} U^H$ holds for $k=1$.
    \end{proof}
    Note that this case is particular because it derives matrix which is
    indeed scalar. 
\item Base Case: $k = 2$, this can be proved by using result of (b). 
    \item Inductive Case: assume that (I.H.)
        \begin{align}
            H_{k-1} \cdots H_1 H_0 = I - U_{(k-1)} L^{-1}_{(k-1)} U^H_{(k-1)}
        \end{align} 
        show that 
        \begin{align}
            H_k H_{k-1} \cdots H_1 H_0 = I - U_{(k)}L^{-1}_{(k)} U^H_{(k)}
        \end{align} 
        holds for lower triangular matrix $L_{(k)}$.
        \begin{proof}
            With application of the result of (c), we have
            \begin{align}
                H_k (H_{k-1} H_{k-2} \cdots H_{1} H_{0}) 
                &= (1 - \frac{1}{\tau_k} u_k u_k^T) (I - U_{(k-1)} L^{-1}_{(k-1)} U^H_{(k-1)}) \\
                &= I - \underbrace{ (U_{(k-1)} | u_k) }_{ U_{(k)} }
                \underbrace{
                \left( \begin{array}{c|c} 
                        L_{(k-1)} & 0 \\ \hline
                        u_k^T U_{(k-1)} & \tau_k
                    \end{array} \right)^{-1}
            }_{ L_{(k)}^{-1} }
                \underbrace{ (U_{(k-1)} | u_k)^{T} }_{ U_{(k)}^T } \\
                &= I - U_{(k)} L_{(k)}^{-1} U_{(k)}^T
            \end{align}
        \end{proof}
    \item By the principle of mathematical induction, it can be concluded that 
        \begin{align}
            H_{k-1} \cdots H_1 H_0 = I - UL^{-1} U^H
        \end{align}
        holds for all $k > 0$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{}
\subsection{Relationship between $m$, $n$ and $r$.}
\begin{align}
    rank(A) = r \leq \min(m, n)
\end{align}

\subsection{Reduced SVD in terms of $U$, $V$ and $\Sigma$.}
\begin{itemize}
    \item $A = U_r \Sigma_r V_r^T$, where 
        $U_r = U_L$, $\Sigma_r = \Sigma_{TL}$ and $V_r = V_L$.
\begin{align}
    A = U\Sigma V^T 
    = (U_L | U_R) 
    \left( \begin{array}{c|c} \Sigma_{TL} & 0 \\ \hline 0 & 0 \end{array} \right)
    (V_L | V_R)^T
    = U_L \Sigma_{TL} V_L^T
\end{align}

    \item $(A^TA)^{-1} = U_r \Sigma_r V_r^T$, where 
        $U_r = V_L^{-T}$, $\Sigma_r = \Sigma_{TL}^{-1}\Sigma_{TL}^{-1}$ and $V_r = V_L^{-T}$.
\begin{align}
    (A^TA)^{-1} &= \big((U\Sigma V^T)^T  U\Sigma V^T \big)^{-1} \\
    &= \big(V \Sigma^T U^T U\Sigma V^T \big)^{-1}  \\
    &= \big(V \Sigma \Sigma V^T \big)^{-1}  \\
    &= \Bigg( (V_L | V_R) 
    \left( \begin{array}{c|c} \Sigma_{TL} & 0 \\ \hline 0 & 0 \end{array} \right)
    \left( \begin{array}{c|c} \Sigma_{TL} & 0 \\ \hline 0 & 0 \end{array} \right)
    (V_L | V_R)^T \Bigg)^{-1} \\
    &= \big( V_L \Sigma_{TL} \Sigma_{TL} V_L^{T} \big)^{-1}  \\
    &= V_L^{-T} \Sigma_{TL}^{-1} \Sigma_{TL}^{-1} V_L^{-1}  
\end{align}

    \item $(A^TA)^{-1} A^T = U_r \Sigma_r V_r^T$, where 
        $U_r = V_L^{-T}$, $\Sigma_r = \Sigma_{TL}^{-1}$ and $V_r = U_L$.
\begin{align}
    (A^TA)^{-1} A^T 
    &= V_L^{-T} \Sigma_{TL}^{-1} \Sigma_{TL}^{-1} V_L^{-1}  V_L \Sigma_{TL}^T U_L^T \\
    &= V_L^{-T} \Sigma_{TL}^{-1} \Sigma_{TL}^{-1} \Sigma_{TL}^T U_L^T \\
    &= V_L^{-T} \Sigma_{TL}^{-1} U_L^T
\end{align}

    \item $A(A^TA)^{-1} = U_r \Sigma_r V_r^T$, where 
        $U_r = U_L$, $\Sigma_r = \Sigma_{TL}^{-1}\Sigma_{TL}^{-1}$ and $V_r = V_L^{-T}$.
\begin{align}
    A (A^TA)^{-1}  
    &= U_L \Sigma_{TL} V_L^T V_L^{-T} \Sigma_{TL}^{-1} \Sigma_{TL}^{-1} V_L^{-1}   \\
    &= U_L \Sigma_{TL} \Sigma_{TL}^{-1} \Sigma_{TL}^{-1} V_L^{-1}   \\
    &= U_L \Sigma_{TL}^{-1} V_L^{-1} 
\end{align}

    \item $A(A^TA)^{-1} A^T = U_r \Sigma_r V_r^T$, where 
        $U_r = I^{m\times r}$, $\Sigma_r = I^{r\times r}$ and $V_r = I^{n\times r}$.
\begin{align}
    A (A^TA)^{-1} A^T
    &= (U_L \Sigma_{TL}^{-1} V_L^{-1}) V_L \Sigma_{TL}^T U_L^T \\
    &= U_L \Sigma_{TL}^{-1} \Sigma_{TL}^T U_L^T \\
    &= U_L  U_L^T \\
    &= I 
\end{align}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\cchi}{\check{\chi}}
\subsection{$\psi_1 = (v_{11} - \delta v_{11}) \cchi_1$ where $\delta v_{11}$ is small.}
\begin{proof}
\begin{align}
    \psi_1 
    = [v_{11} \cchi_1]_* 
    = \frac{v_{11} \cchi_1}{1+\epsilon_*}
    = (1 - \frac{\epsilon_*}{1+\epsilon_*}) v_{11}\cchi_1
    = (v_{11} - \frac{\epsilon_*v_{11}}{1+\epsilon_*}) \cchi_1
    \equiv (v_{11} + \delta v_{11}) \cchi_1
\end{align}
where 
\begin{align}
    \delta v_{11}  &= - \frac{\epsilon_*v_{11}}{1+\epsilon_*} 
    \text{, where } |\epsilon_*| \leq \bu
\end{align}
Then 
\begin{align}
    |\delta v_{11}| &= |- \frac{\epsilon_*}{1+\epsilon_*} v_{11}|  \\
    &= \frac{|\epsilon_*|}{|1+\epsilon_*|} | v_{11}| \\
    &\leq \frac{\bu}{|1+\epsilon_*|} | v_{11}| \\
    &\leq \frac{\bu}{1+|\epsilon_*|} | v_{11}| \\
    &\leq \frac{\bu}{1+|\epsilon_*| -|\epsilon_*| - \bu} | v_{11}| \\
    &= \frac{\bu}{1 - \bu} | v_{11}| 
    = \gamma_1 | v_{11}| 
\end{align}
Hence, we proved that 
\begin{align}
    \psi_1 = (v_{11} - \delta v_{11}) \cchi_1 \text{ where }
|\delta v_{11}| = \gamma_1 | v_{11}| 
\end{align}
\end{proof}

\subsection{}
\begin{proof}
Start from dot product. We directly use the corollary in notes. 
\begin{align}
    d \overset{\Delta}{=} [u_{12}^T \check{x_2}] = (u_{12}^T + \delta u_{12}^T) \check{x_2}
    \text{ , where } \delta u_{12}^T \leq \gamma_k |u_{12}^T| \leq \gamma_{k+1} |u_{12}^T|
\end{align}
Then we consider the subtraction step with Standard Computational Model (SCM).
\begin{align}
    s \overset{\Delta}{=} [\psi_1 - d] 
    = (\psi_1 - d) (1+\epsilon_-)
    = (\psi_1 - d) + (\psi_1 - d) \epsilon_-
    \equiv (\psi_1 - d) + \delta \psi_1 
    = (\psi_1 + \delta \psi_1 ) - d
\end{align}
where 
$|\delta \psi_1| = |(\psi_1 - d) \epsilon_-| \leq | \psi_1  \epsilon_-|
    =  |\epsilon_-| | \psi_1 | \leq \bu |\psi_1| $.
Then we consider the division step with Alternative Computational Model (ACM).
\begin{align}
    &\cchi_1 = [\frac{s}{v_{11}}]  = \frac{s}{v_{11}} (1+\epsilon_/)_{-1} 
    =  \frac{s}{v_{11}(1+\epsilon_/)}
\end{align}
which can be rewritten as  
\begin{align}
    (v_{11}+\delta v_{11}) \cchi_1  = s
\end{align}
where $ |\delta v_{11}| = |\epsilon_/ v_{11}| \leq |\epsilon_/|\cdot |v_{11}| \leq \bu |v_{11}| $.
Then we have 
\begin{align}
    (v_{11}+\delta v_{11}) \cchi_1  
    &= (\psi_1 + \delta \psi_1 ) - d  
    = (\psi_1 + \delta \psi_1 ) - (u_{12}^T + \delta u_{12}^T) \check{x_2}
\end{align}
where 
\begin{align}
    |\delta v_{11}| \leq \bu  |v_{11}|,\ 
    |\delta \psi_1| \leq \bu |\psi_1|,\ 
    |\delta u_{12}^T| \leq \gamma_{k+1}|u_{12}^T|
\end{align}
\end{proof}

\newpage
\subsection{Induction for the desired result}
To prove the desired result, let us do mathematical induction on $k$ (the
iteration of algorithm).
\begin{itemize}
    \item Base Case: $k = 0$
        \begin{proof}
        In this case, $U^{0\times 0}$ does not participate in
        computation, $v_{11}$ is the right-bottom-most element, and no
        pre-computed $\check{x}_2$. Hence, it is natural that
        \begin{align}
            (v_{11} + \delta v_{11}) \cchi_1 = (\psi_1+\delta\psi_1)
        \end{align}
        which indicates that $(U + \Delta U) \check{x} = (y + \delta y)$ holds
        for $k=0$. (first iteration)
    \end{proof}
\item Inductive case: Assume that at $p$-th iteration ($p > 0$), pre-computed $\check{x}_{2(p)}$ satisfy
        \begin{align}
            (U_{(p)} + \Delta U_{(p)}) \check{x}_{2(p)} = (y_{2(p)} + \Delta y_{2(p)})
        \end{align}
        where $|\Delta U_{(p)} | \leq \gamma_n |U_{(p)}|$ 
        and $|\delta y_{2(p)}| \leq \bu |y_{2(p)}|$,
        (I.H.) holds. \\
        Show that for the pre-computed $\check{x}_2$ at next iteration
        $\check{x}_{2(p+1)} = \left( \begin{array}{c} \cchi_{1(p)} \\\hline
                \check{x}_{2(p)}  \end{array} \right)$ 
         and 
        $y_{2(p+1)} = \left( \begin{array}{c} \psi_{1(p)} \\\hline
                y_{2(p)}  \end{array} \right)$ 
        \begin{align}
            (U_{(p+1)} + \Delta U_{(p+1)}) \check{x}_{2(p+1)} = (y_{(p+1)} + \Delta y_{(p+1)})
            \label{pp1conclusion}
        \end{align}
        where $|\Delta U_{(p+1)} | \leq \gamma_n |U_{(p+1)}|$ 
        and $|\delta y_{2(p+1)}| \leq \bu |y_{2(p+1)}|$,
        holds for some upper triangle $U_{(p+1)}$.
        \begin{proof}
            In terms of result of (b), we have
            \begin{align}
                (v_{11(p)}+\delta v_{11(p)}) \cchi_{1(p)}  
                + (u_{12(p)}^T + \delta u_{12(p)}^T) \check{x}_{2(p)}
                = (\psi_{1(p)} + \delta \psi_{1(p)} ) 
            \end{align}
            where $\delta v_{11(p)} \leq \bu|v_{11(p)}|$, 
            $\delta \psi_{1(p)} \leq \bu |\psi_{1(p)}|$ ,
            and $\delta u^T_{12(p)} \leq \gamma_{p+1} |u^T_{12(p)}| $.
            \\ Combined with (I.H.)
            \begin{align}
            (U_{(p)} + \Delta U_{(p)}) \check{x}_{2(p)} = (y_{2(p)} + \Delta y_{2(p)})
            \end{align}
            we have
            \begin{align}
                \left( \begin{array}{c|c}
                        v_{11(p)}+\delta v_{11(p)} &  u_{12(p)}^T + \delta u_{12(p)}^T \\\hline
                        0 & U_{(p)} + \Delta U_{(p)} 
                    \end{array} \right)
                \left( \begin{array}{c} \cchi_{1(p)} \\\hline \check{x}_{2(p)} \end{array} \right)
                = 
                \left( \begin{array}{c} \psi_{1(p)} + \delta \psi_{1(p)}  \\\hline
                       y_{2(p)} + \Delta y_{2(p)} \end{array} \right)
            \end{align}
            where $|\Delta U_{(p)} | \leq \gamma_n |U_{(p)}|$,
            $\delta v_{11(p)} \leq \bu|v_{11(p)}| \leq \gamma_n |v_{11(p)}|$, 
            $\delta u^T_{12(p)} \leq \gamma_{p+1} |u^T_{12(p)}| \leq \gamma_{n} |u^T_{12(p)}|$
            \\ and $\delta \psi_{1(p)} \leq \bu |\psi_{1(p)}|$ ,
            $|\delta y_{2(p)}| \leq \bu |y_{2(p)}|$. (Note that $0 \leq p \leq n-1$)
            \\ which is equivalent to
            \begin{align}
                \Bigg(
                \underbrace{
                \left( \begin{array}{c|c}
                        v_{11(p)} &  u_{12(p)}^T  \\\hline
                        0 & U_{(p)} 
                    \end{array} \right)
            }_{U_{(p+1)}}
                +
                \underbrace{
                \left( \begin{array}{c|c}
                        \delta v_{11(p)} &   \delta u_{12(p)}^T \\\hline
                        0 & \Delta U_{(p)} 
                    \end{array} \right)
            }_{\Delta U_{(p+1)}}
                \Bigg)
                \underbrace{
                \left( \begin{array}{c} \cchi_{1(p)} \\\hline \check{x}_{2(p)} \end{array} \right)
            }_{\check{x}_{2(p+1)}}
                &= 
                \Bigg(
                \underbrace{
                \left( \begin{array}{c} \psi_{1(p)} \\\hline
                       y_{2(p)}  \end{array} \right)
            }_{y_{2(p+1)}}
               +
                \underbrace{
                \left( \begin{array}{c}  \delta \psi_{1(p)}  \\\hline
                       \Delta y_{2(p)} \end{array} \right)
            }_{\delta y_{2(p+1)}}
                \Bigg) \\
                (U_{(p+1)} + \Delta U_{(p+1)}) \check{x}_{2(p+1)} 
                &= (y_{2(p+1)} + \delta y_{2(p+1)})
            \end{align}
            where $|\Delta U_{(p+1)} | \leq \gamma_n |U_{(p+1)}|$ 
            and $|\delta y_{2(p+1)}| \leq \bu |y_{2(p+1)}|$.
            \\ Hence, we sucessfully proved \eqref{pp1conclusion}.
        \end{proof}
    \item By principle of mathematical induction, it is proved w.l.o.g. that 
        \begin{align}
            (U + \Delta U) \check{x} = (y + \delta y)
            \text{ , where } |\Delta U | \leq \gamma_n |U| 
            \text{ and } |\delta y| \leq \bu |y|
        \end{align}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% General Documentation ends
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
