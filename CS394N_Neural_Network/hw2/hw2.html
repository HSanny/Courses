<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <title>CS394N Neural Networks:  Handwritten Digit Recognition</title>
	<meta http-equiv="content-type" content="text/html;charset=iso-8859-1"
  </head>
  <body>
    <h1>Handwritten Digit Recognition with Backpropagation</h1>

    <h3>Network</h3> 
<p>Design a backpropagation network that receives an 8 by 8 bitmap of 
a handwritten digit as input and outputs the classification of this digit 
as output. The input layer has 64 units, one for each pixel. The output 
layer consists of one unit for each digit. You can have as many hidden 
layers with as many units as you want.

<h3>Training data</h3>
<p> You will design 8 by 8 bitmap representations
  for each digit, and concatenate the representations into 64-unit input vectors.
  The output of the network is a 10-unit vector, where the units correspond to
  digits 0-9. Design 4-10 example patterns for each digit. Try to come up with
  a training set that would be likely to give good generalization to digits written
  in different styles, and also to noisy digits.
<p>You can reuse the digits that
    you designed for the SOM/LVQ homework. However, you will need to convert
  them into the data file format (.dat file extension) of Emergent, the neural
    network simulator we will be using for this homework. This can be done by
    using the following python script:
<blockquote>
  <p>	<code>python convert_pat.py training-digits.pat hwk2-train.dat</code></p>
</blockquote>


<h3>Testing data</h3> 

<p> The test pattern file
<a href="emergent/hwk2-test.dat"><tt>hwk2-test.dat</tt></a> contains several examples
of inaccurate, noisy, distorted, and otherwise corrupted input patterns. 
The objective is to find out how well the network generalizes into real world
data, such as the patterns from the <a href="emergent/nist.dat">NIST dataset</a>.
Be sure you do not use any of the test patterns during training!  Typically
during testing, the output of the network is no longer a clear-cut 
classification as was the case for the training vectors.  The output 
unit activities stand for the likelihoods of each classification.

<h3>Tools</h3> 

<h4>Emergent</h4>
<p> The Emergent simulator package is installed on the CS Linux machines, you just need to run the command <tt>emergent</tt>.    

<p>Binary packages are also available from the 
project webpage for Ubuntu, Mac and Windows.  You may also build the package yourself if you wish.  Links to the binaries and instructions 
for building from source are available <a href="https://grey.colorado.edu/emergent/index.php/Main_Page">here</a>.

The following <a href="https://www.youtube.com/watch?feature=player_embedded&v=ALbdeDKJesg">video</a> offers a high level overview of the Emergent interface, or you may read the documentation at <a href="https://grey.colorado.edu/emergent/index.php/Using_emergent"><tt>https://grey.colorado.edu/emergent/index.php/Using_emergent</tt></a>.


<h3>Experiments</h3>
<p> Start by designing one set of input examples.  Train the network until it has
  learned all training patterns within 20% accuracy (i.e. the correct output
   unit is on with activity &gt; 0.8, and all the other units are off with 
  activity &lt; 0.2). Do not start with too many hidden units. A network of 
  64-20-10 with 40 training patterns should train reasonably well in about 50
  cycles using standard backpropagation with a learning rate of 0.2 and momentum
  of 0.9.

<p>Detailed instructions on running Emergent for this project are available <a href="instructions.html">here</a>.

<p> Then see how well it performs on the test set. Look at those 
examples that it identifies correctly and also those that it does not. 
Do you see any reason for its behavior?  You will probably see several 
problems with its training.  Try to fix some of the problems by adding 
more training patterns.  Test it again on the same test set.  Did the 
performance improve?

<h3>Optional</h3>
<p> Finally, if you want, you can test the performance on your
network on a third set of patterns, <a href="emergent/nist.dat">
<tt>emergent/nist.dat</tt></a>. This is a collection of 2992 test patterns 
from a publicly-available database at the National Institute of Standards 
and Technology (NIST). Performance on this data set should give you a 
pretty good idea how well your network performs in the real world.  Of 
course, if you want to see how well your network can really perform, you 
should use a portion of the NIST set for a training set, a different 
portion as a validation set (to decide when to stop training, if needed), 
and the remainder of the patterns as a test set.  You should then repeat 
the training and testing over many trials with different splits of the data 
into sets, and average the final results.

<h3>Report</h3> 
<p> Write a clear, concise one page report about the experiments you
did, especially analyzing issues such as: How hard is it to train the
network to do the task? What kind of architectures and parameter
settings work the best, and why? What kind of deviations from the
training patterns causes the most trouble and why? Did you see
evidence of overtraining? Do you think straightforward backpropagation
is the right approach to this task? If not, how could you improve it
to make it more powerful and flexible? How does your backprop network perform
  compared to SOM/LVQ? Turn your report, the final saved network file, result
  files, and your training pattern files electronically.

<h3>Turning In</h3>
<p>Turn in on Canvas</p>
<h3>Frequently Asked Questions</h3>
There is a set of <a href="faq.shtml">FAQ's</a> for this homework.  Before asking
a question, please see if you can find an answer there.

    <hr>
<p>Originally By Marshall Mayberry.</p>
<p>Updated and adapted to use Emergent simulator by Kim Houck, Fall 2014</p>
</html>
