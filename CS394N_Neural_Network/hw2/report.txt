%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CS394N: HOMEWORK 02 BACKPROPAGATION
%% This file includes my response to homework questions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
EID: XL5224
NAME: XIN LIN

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Q1. How hard is it to train the network to do the task?
A: Even though neural network is an universal function approximator, the
optimal solution for network weights has symmetry. The symmetry may confuse us
which approximate position the optimal weights of network resides in.
Secondly, artificial neural network also suffers the issue of local
optimality.  It is time-consuming to judge if one local minima is global
minima. This is because we have to do multiple runs with random initialization
to get network that looks good enough (even though it is still not global
optima). Limited resolution is also a problem. It limits the amount of
information our intelligent system could address and hence more confusion
between digits will be made. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Q2. What kind of architectures and parameter settings work the best, and why? 
A: Typically, two-layer neural network does not provide very good performance.
About 3-5 layers help significantly with the performance. In addition to that,
the first hidden layer should have more units (2-3 times) than the input layer. 
This is because the first layer plays the role of feature extractor and it is
not suitable to compress the information in that layer. (such that the later
layer can better base on the first hidden layer.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Q3. What kind of deviations from the training patterns causes the most trouble
and why? 
A: As previously mentioned, the limited resolution causes great deviation in
training patterns. More specifically, the position of 1 (black dot) provides
little information for what the digit is.  In addition to that, high-level
characteristics like ring in the bitmap may fail to convey the identifying
information.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Q4. Did you see evidence of overtraining? 
A: Yes, in one experimentation, early stop (training with less iterations
limit) improves the performance of network. That is to say, the extra
epoch are playing an role of overtraining. Specifically, when I use the
self-created dataset, more training epochs reach higher sse. 
Also, early stopping is a commonly used approach to prevent overfitting
problem, as well as regularization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Q5. Do you think straightforward backpropagation is the right
approach to this task? If not, how could you improve it to make it more
powerful and flexible? 
A: To the best of my knowledge, backpropagation is the best approach to
upgrade the weight of artificial neural network among all existing methods
that computes gradients. It requires linear complexity to the number of
network connections. Note that numerical difference needs quadratic complexity
to the number of network connections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Q6. How does your backprop network perform compared to SOM/LVQ? 

A: 

Turn your report, the final saved network file, result files, and your
training pattern files electronically.
