%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  THIS TEX FILE IS TO GENERATE PDF FILE FOR 
%%% 
%%%  COPYRIGHT (C) JIMMY LIN, 2013, UT AUSTIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,a4paper]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  PACKAGES USED IN THIS TEX SOURCE FILE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry,amsthm,amsmath,graphicx,amssymb,fancyheadings}
\usepackage[]{mcode}
\usepackage[colorlinks,
            linkcolor=blue,
            anchorcolor=red,
            citecolor=green
            ]{hyperref}
% for my mac
\IfFileExists{/Users/JimmyLin/.latex/UTA_CS/JS.sty}{ 
    \usepackage{/Users/JimmyLin/.latex/UTA_CS/JS}
    \usepackage{/Users/JimmyLin/.latex/UTA_CS/JSASGN}
}{} 
% for UT's linux machine
\IfFileExists{/u/jimmylin/workspace/Configs/latex/UTA_CS/JS.sty}{
    \usepackage{/u/jimmylin/.latex/UTA_CS/JS} 
    \usepackage{/u/jimmylin/.latex/UTA_CS/JSASGN}
}{} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% MACROS CONTAINING THE FILE INFORMATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\COURSE}{EE381V Large Scale Optimization}
\renewcommand{\LECTURER}{Sujay Sanghavi}
\renewcommand{\SECTION}{17350}
\renewcommand{\TASK}{Problem Set 3}
\renewcommand{\RELEASEDATE}{October 08, 2014}
\renewcommand{\DUEDATE}{October 16, 2014}
\renewcommand{\TIMECONSUME}{5 hours}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% DOCUMENTATION STARTS FROM HERE 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE PAGE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
    \maketitle
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CONTENT PAGE: TABLEOFCONTENTS, LISTOFTABLES, LIST OF FIGURES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\contentsname}{Table of Contents}
\begin{center} 
    \tableofcontents 
    %\listoftables 
    \listoffigures
\end{center}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% GENERAL DOCUMENTATION BEGINS 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{remark}{Remark}
\part{Written Problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Written Problems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{section}{0}
\renewcommand{\thesubsection}{(\alph{subsection})}
\section{Gradient descent with diminising step size}
\begin{proof}
    Start from analyzing gradient descent with step size $t$.
    \begin{align}
        f(x^+) &= f(x + t \Delta x) \\
        &= f(x - t\nabla f(x)) \\
        &= f(x) - t|| \nabla f(x) ||_2^2 + \frac{\nabla^2 f(x)}{2} t^2 || \nabla f(x) ||_2^2 \\
        &\leq f(x) - t || \nabla f(x) ||_2^2 + \frac{L}{2} t^2 || \nabla f(x) ||_2^2 
    \end{align}
    The last step is natural derivation of $L$-Lipschitz gradient of
    $f(\cdot)$. \\
    Set gradient of RHS to zero and get the minimum of RHS when $t = \frac{1}{L}$, then we have
    \begin{align}
        f(x^+) &\leq f(x) - \frac{1}{2L} || \nabla f(x) ||_2^2  \label{leq}
    \end{align}
    which can be rewritten as
    \begin{align}
        f(x^{(k+1)}) - f(x^{(k)}) &\leq - \frac{1}{2L} || \nabla f(x^{(k)}) ||_2^2  \label{leq}
    \end{align}
    Then we recursively apply \eqref{leq} for infinity many step with summation,
    \begin{align}
        f(x^{(+\infty)}) &\leq f(x^{(0)}) - \sum_{k=0}^{+\infty} \frac{1}{2L} || \nabla f(x^{(k)}) ||_2^2 
    \end{align}
    Since $f(\cdot)$ has finite minimum $f_{min}$, and then
    \begin{align}
        f_{min} \leq f(x^{(+\infty)}) &\leq f(x^{(0)}) - \sum_{k=0}^{+\infty} \frac{1}{2L} || \nabla f(x^{(k)}) ||_2^2  
    \end{align}
    That is 
    \begin{align}
      \sum_{k=0}^{+\infty} \frac{1}{2L} || \nabla f(x^{(k)}) ||_2^2    &\leq f(x^{(0)}) - f_{min} 
    \end{align}
    Since $\forall k,\ t^{(k)} < \frac{2}{L}$, then
    \begin{align}
        \sum_{k=0}^{+\infty} \frac{1}{4} t^{(k)} || \nabla f(x^{(k)}) ||_2^2 <
      \sum_{k=0}^{+\infty} \frac{1}{2L} || \nabla f(x^{(k)}) ||_2^2 
      \leq f(x^{(0)}) - f_{min} 
    \end{align}
    Then in terms of the given fact, we have 
    \begin{align}
       \frac{1}{4} || \nabla f(x^{(k)}) ||_2^2 \rightarrow 0 \text{ a.k.a } 
        \nabla f(x^{(k)}) \rightarrow 0
    \end{align}
    which indicates gradient descent eventually converges to a stationary point.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Gradient descent and non-convexity}
Apply eigenvalue decomposition for $f(x) = x^T Q x$, then we have $g(y) =
f(x)$, where $g(y) = y^T \Lambda y$ and $\Lambda = diag(\lambda_0, ...,\lambda_n)$
are diagonal matrix with eigen values. Then we just need to analyze the
convergence on $g(y)$ since $g(y)$ is just a coordinate change of $f(x)$. \\
Start from gradient descent update for $g(y)$,
\begin{align}
    \nabla g(y) &= \Lambda y \\
    y^{(k+1)} &= y^{(k)} - \eta \Lambda y^{(k)}
\end{align}
Let us look at update on each dimension individually, 
\begin{align}
    y^{(k+1)}_i &= y^{(k)}_i - \eta \lambda_i y_i^{(k)} \\
    &= \underbrace{(1 - \eta \lambda_i)}_{c_i} y_i^{(k)}
\end{align}
Let $c_i = (1 - \eta \lambda_i)$, it is obvious that for non-optimal $y_i$, 
\begin{align}
    \text{when } \lambda_i = 0&,\ c_i = 1,\ g(y) \text{ does neither converge and diverge. } \\
    \text{when } \lambda_i > 0&,\ c_i < 1,\ g(y) \text{ diverge if $|c_i| < 1$, and converge 
    if $c_i < -1$. } \\
   \text{when } \lambda_i < 0&,\ c_i > 1,\ g(y) \text{ always diverge whatever
       step size $\eta$ is. }
\end{align}
Hence, the characterization of those initial points, for which gradient with
any step size will diverge, is those points that are already optimal on
dimension $i$ ($x_i$ is already optimal) for eigen value on dimension $i$ is
negative ($\lambda_i < 0$). 
Briefly, initial points $x$ where $x_i$ is optimal for all $i,\ \lambda_i <
0$. \\
For other initial points, they will diverge with inappropriate choice of step size.
Starting from these initial points can still converge to the global optima
with arbitrary step size if and only if for one initial point $x$, all $x_i$
satisfy: 
(a) already optimal if $\lambda_i \leq 0$. 
(b) $-1 < 1-\eta \lambda_i < 1$ if $\lambda_i > 0$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Jacobi Method}
Prove that, for a convex continuously differentiable $f$, and a step size
$\alpha = 1/n$ where $n$ is the number of coordinates, the next iterates of
the Jacobi method produces a lower function value than $x$, provided $x$ does
not already minimize the function.

\begin{proof}
    Let $x^*_i = (x_1, ..., x_{i-1}, \bar{x}_i, x_{i+1}, ..., x_n)$. Then we
    attempt to represent $x^{+}$ as convex combination of $n$ points
    ($x^*_i,\ i = 1,...,n$).
    \begin{align}
        x^{+} &= x + \alpha (\bar{x} - x) \\
        &= x + \frac{1}{n} (\bar{x} - x) \\
        &= (1 - \frac{1}{n})x +  \frac{1}{n}\bar{x} \\
        &= (\frac{n-1}{n})x +  \frac{1}{n}\bar{x} \\
        &= \left( \frac{n-1}{n}x_1 + \frac{1}{n} \bar{x}_1 ,
                \frac{n-1}{n}x_2 + \frac{1}{n} \bar{x}_2 ,
                    ... ,
                \frac{n-1}{n}x_n + \frac{1}{n} \bar{x}_n \right) \\
            &= \sum_{i=1}^{n} \frac{1}{n} x_i^* 
    \end{align}
    which presents us the convex combination. Then
    \begin{align}
        f(x^+) &\leq f(\sum_{i=1}^n \frac{1}{n} x_i^*)  \\
        &\leq \sum_{i=1}^n \frac{1}{n} f (x_i^*) && f \text{ is convex } \\
        &\leq \sum_{i=1}^n \frac{1}{n} f (x) && \forall i, f(x_i^*) \leq f(x) \\
        &= f(x)
    \end{align}
    where equality holds when $\forall i,\ x_i = \bar{x}_i$ ($x^+ = x$), that is, point
    $x$ does already minimize the function. And in other cases, the next
    iterate of the Jacobi method produces a lower function value than $x$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Step size in Newton}
\subsection{Values of $t$ obtain global convergence}
For update with Newton step $\Delta x = \nabla^2 f(x)^{-1} \nabla f(x)$ 
\begin{align}
    f(x^+) &= f(x + t\Delta x) \\
    &= f(x) + \nabla f(x)^T \cdot (t \Delta x) + (t\Delta x)^T \nabla^2f(x) (t\Delta x) \\
    &= f(x)-t\nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) + t^2 \nabla f(x)^T
    \nabla^2 f(x)^{-1} \nabla f(x) \\
    & = f(x) + (t^2 - t) \lambda^2(x) \\
    & = f(x) + t (t - 1) \lambda^2(x)
\end{align}
where $\lambda^2(x) = \nabla f(x)^T \nabla^2 f(x)^{-1}\nabla f(x) $. Then in order to
obtain global convergence to $x^* = 0$,
\begin{align}
    f(x^+) - f(x) = t (t - 1) \lambda^2(x) < 0
\end{align}
Since $f(x) = || x ||^3$ is strongly convex (obvious) with unique minima, then
\begin{align}
    \lambda^2(x) = \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) 
    \geq \frac{1}{M} || \nabla f(x)||_2^2 \geq 0
\end{align}
Then since the fixed step size $t > 0$,
\begin{align}
    t - 1 < 0 
\end{align}
That is 
\begin{align}
    0 < t < 1 
\end{align}
Hence, for $ 0 < t < 1 $, we can obtain global convergence to the minimum of
$f(x) = || x ||^3$.

\subsection{Reason that convergence is not quadratic}
The convergence is not quadratic because for $ 0 < t < 1 (t \not = 1)$, the
analysis on textbook for rate of convergence does not hold any more.
Specifically, 
\begin{align}
    || \nabla f(x+) ||_2 
    &= || \int_{0}^{t} (\nabla^2f(x+\eta\Delta x) - \nabla^2f(x)) \Delta x d\eta ||_2 \\
    &\leq \frac{Lt}{2} || \Delta x ||_2^2 \\
    &\leq \frac{Lt}{2m^2} || \nabla f(x) ||_2^2 
\end{align}
which is not consistent with the formula of quadratic convergence. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Composite functions}
\subsection{Run gradient descent on $f$ and $g$}
Show that the entire sequence of iterates will then be the same. 
\begin{proof}
    The gradient descent direction for $f(x)$ and $g(x)$ are respectively
    \begin{align}
        \Delta x_{f(x)} &= \nabla_x f(x) \\
        \Delta x_{g(x)} &= \nabla_x g(x) 
        = \nabla_x \phi(f(x)) = \nabla_{f(x)} \phi(f(x)) \nabla_x f(x)
    \end{align}
    Apply direction to update rule, we have
    \begin{align}
        x^{+}_{(f)} &= x + t^*_{(f)} \nabla_x f(x) \\
        x^{+}_{(g)} &= x + t^*_{(g)} \nabla_{f(x)} \phi(f(x)) \nabla_x f(x)
    \end{align}
    where the optimal step size for $f(x)$ is 
    \begin{align}
        t^*_{(f)} &= \argmin_t f(x+t\nabla_x f(x))
    \end{align}
    and the optimal step size for $g(x)$ is 
    \begin{align}
        t^*_{(g)} &= \argmin_t g(x+t\nabla_{f(x)} \phi(f(x)) \nabla_x f(x)) \\
        &= \argmin_t \phi\big(f(x+t\nabla_{f(x)} \phi(f(x)) \nabla_x f(x))\big) \\
        &= \argmin_t f(x+t \nabla_{f(x)} \phi(f(x)) \nabla_x f(x))
        && \phi(\cdot) \text{ is increasing function} \label{phiexist}
    \end{align}
    Now we observe that both step size $t^*_{(f)}$ and $t^*_{(g)}$ can be seen
    as exact line search of $f(\cdot)$ on point $x$ towards direction
    $\nabla_x f(x)$. (Note that $\phi(f(x))$ is a scalar.) But two step size
    has different scale due to the existence of $\phi(f(x))$ on
    \eqref{phiexist}. Hence, we have 
    \begin{align}
        t^*_{(f)} = t^*_{(g)} \nabla_{f(x)}\phi(f(x))
    \end{align}
    Thus, 
    \begin{align}
        x^{+}_{(f)} &= x + t^*_{(f)} \nabla_x f(x) \\
        &= x +  t^*_{(g)} \nabla_{f(x)}\phi(f(x)) \nabla_x f(x) \\
        &= x^{+}_{(g)}
    \end{align}
    which indicates that one iteration of gradient descent method on
    $f(\cdot)$ and $g(\cdot)$ starting with the same point $x$ (arbitrarily)
    will go to the same point $(x^{+}_{(f)} = x^{+}_{(g)})$. Recursively
    applying this derivation, it is proved that the entire sequence of
    iterates will then be the same. 
\end{proof}

\newpage
\subsection{Run Newton method on $f$ and $g$}
Not true for Newton Method.
\begin{proof}
    The Newton step for $f(x)$ and $g(x)$ are respectively
    \begin{align}
        \Delta x_{f(x)} &= \nabla_x^2 f(x)^{-1}\nabla_x f(x) \\
        \Delta x_{g(x)} &= \nabla_x^2 g(x)^{-1} \nabla_x g(x)  \\
        &= \bigg(\nabla_{f(x)} \phi (f(x)) \nabla^2_x f(x) + 
        \nabla^2_{f(x)} \phi (f(x))I \bigg)^{-1}
        \nabla_{f(x)} \phi (f(x)) \nabla_x f(x) 
    \end{align}
    where $\nabla_{f(x)} \phi (f(x))$ is a scalar. 
    According to matrix inversion lemma, 
    \begin{align}
        \Delta x_{g(x)} 
        &= \nabla_{f(x)} \phi (f(x)) 
        \underbrace{\bigg( I - \nabla_x^2 f(x)^{-1} \nabla^2_{f(x)} \phi (f(x)) \big(
        I - \nabla_x^2 f(x)^{-1} \nabla^2_{f(x)} \phi (f(x)) \big)
        \bigg)}_{S}
        (\nabla_x^2 f(x))^{-1}\nabla_x f(x) 
    \end{align}
    which indicates that $\Delta x_{f(x)}$ and $\Delta x_{g(x)}$ have
    different direction since $S$ is non-trivial. Hence, the exact line search on these two function
    from the same initial point will cause different result. And thus, the
    entire sequence of iterates will not be the same.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% General Documentation ends
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
