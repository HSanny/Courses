<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"> 
<html> 
  <head> 
    <title>Reinforcement Learning</title> 
    <link rel=stylesheet 
     href="http://www.cs.utexas.edu/users/nn/nn.css"
     type="text/css">  
  </head> 
  <body> 
  <h1>Reinforcement Learning<br> 
        (due 11:59pm, Oct 22, 2013)</h1> 
 
<P> 
In this assignment, you will be implementing a function approximator for the Q-Learning reinforcement learning algorithm in the OpenNERO platform. It will abstract a more fine-grained version of the problem space, making the problem tractable for the Q-Learner. <a href="http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html">Q-Learning</a> is an off-policy reinforcement learning (RL) algorithm. The current implementation of Q-learning in python (CustomRLAgent in agent.py) stores Q-values in tabular form. The tabular implementation does not scale with increasing state-space.   
<P> 
 
<H2>The OpenNERO Platform</H2> 
<p>The following steps should help get you up and running in OpenNERO:

<ol>
  <li>Go to <a href="http://www.cs.utexas.edu/facilities/public-labs">one of the public labs</a>. In Linux labs, OpenNERO will be installed in /projects/nn/cs343/opennero. In Mac and Windows labs, you can install it on your own machine (next line).</li>
  <li>Install it on your own machine by downloading one of the <a href="http://code.google.com/p/opennero/downloads/list">prebuilt binaries</a></li>
  <li><a href="http://code.google.com/p/opennero/wiki/BuildingOpenNero">Build OpenNERO</a> on your own machine using the source code.</li>
</ol>

<h2>Creating your agent</h2>
<p>
To create your RL agent, open the <a href="http://code.google.com/p/opennero/source/browse/trunk/mods/Maze/agent.py">agent.py</a> file (located in <tt>mods/Maze</tt>). Just as in HW 1, you will be creating your own agent. However, in this case, you will inherit from the AgentBrain class, since there is no need to highlight a final path.</p>

Create a new class called MyRLAgent by copying the existing CustomRLAgent. Verify that your agent works correctly by comparing its behavior to the built-in RL agent in the coarse version.

<h3>Configuration file</h3>
<p>Once you've created your agent, you'll need to add it to the configuration file for the OpenNERO Maze mod. To do this, locate the .xml file CustomRLRobot.xml (<tt>Maze/data/shapes/character/CustomRLRobot.xml</tt>) and change the following line:

<pre>

 &lt;Python agent=&quot;Maze.agent.CustomRLAgent(0.8, 0.8, 0.1)&quot; /&gt;
</pre>

to:
<pre>
 &lt;Python agent=&quot;Maze.agent.MyRLAgent(0.8, 0.8, 0.1)&quot; /&gt;
</pre>

Once you've configured this line, pressing the <tt>Q-learning</tt> button in the Maze mod will run your agent. Three parameters are passed to the agent which you might want to tweak during training -
<ol>
  <li> gamma reward discount factor (varies between 0 and 1) - keep this around 0.8
  <li> alpha learning rate (varies between 0 and 1) - start with 0.4
  <li> epsilon parameter for the epsilon-greedy policy (varies between 0 and 1) - A high epsilon means more exploration. Keep this high at start (0.9) and slowly reduce this as agent learns more and more. Once the agent is full-trained, you can make this parameter 0. 
</ol>


<h3>The fine-grained maze and function approximators</h3>
<p>Then try running your agent on the fine-grained version. This environment divides the world into much smaller steps (64x64), resulting in a substantially larger state space. You'll likely notice that your agent is not performing as well with the larger state. In such cases, it's helpful to implement a function approximator that abstracts the large state space into something more manageable. You will implement two such approximators.</p>

<h4>Tiling Approximator</h4>
<p>Begin by implementing a simple tiling approximator. This function should simply map the current state back to the standard 8x8 space.</p>
For the simple tiling approximator, think of it this way - lets say you have got just 8x8 memory i.e you can just store 64 value function entries. Now due to the limited memory, you divide the 64x64 world grid into 64 tiles of size 8x8 (just like in the figure below where each one of the four quadrants encapsulates 8x8 states). Having such a coarse tiling means that each 8x8 states falling into a single tile will have the same value function. Of-course this does not help you much in terms of learning (and your agent might wander around aimlessly), because nearby states lying in a single tile are not distinguishable. But such coarse tiling does saves you space. However, do not spend too much time in training your agent. 
For further details, refer to <a href="http://www.cs.utexas.edu/users/risto/cs343/private/hw-last-year/hw3/hw3_faq.html">FAQs page</a>. 

As a next step, you will implement nearest-neighbor approximator in order to distinguish the states falling in a single tile. 
<h4>Nearest-Neighbors Approximator</h4>
<p>Nearest-neighbors function approximator should interpolate between the values of the three nearest 8x8 locations. Note that each location should only be interpolated if it is reachable&ndash; don't interpolate across walls! Below is an example of how the nearest-neighbors approximator should work.</p>


<figure>
<img src="hw3_interpolation_example_aditya.png" width="873" height="512" />
<figcaption>Fig1. Nearest Neighbor Approximator. Value of a given state/location is computed by taking weighted sum of 3 nearest neighbors. Equations for tiling approximator can be derived by considering only one nearest neighbor. Note, since OpenNERO environment is deterministic, Value(Y, Left) = Value(X). </figcaption>
</figure>

<p>In the figure, your agent is currently at state Y and is considering taking the action to move left to X. The function approximator would calculate the Q-value at X by interpolating the values of A, B, and C based on their Euclidean distance to X. For instance, X is 3.5 rows up and 1.5 rows to the left of A, so its distance is <tt>sqrt(3.5^2 + 1.5^2)</tt>. Note that since the tiling of the approximator does not line up perfectly with the fine-grained space, every node in the fine-grained (64x64) space will be at least 0.5 rows and columns away from the nearest approximator (8x8) tile.</p>

<p>If X has a larger value than the tiles below or to the right of Y, (Y, Left) will be chosen as the epsilon-greedy (state, action) pair, and that is the value we must use to update the approximator tiles (A, B, and C). To do this, we simply take the reward received at X and perform the standard Q-learning update to all the approximator tiles.</p>

<p>In order to detect walls, your approximator can call <tt>get_environment().maze.walls</tt> to access the set of tuples containing the walls. The format for the walls set is ((r1,c1), (r2,c2)). For instance, to check if there is a wall between (3,4) and (3,5), your approximator could ask:</p>

<pre>
if ((3,4), (3,5)) in get_environment().maze.walls:
    print "There is a wall between (3,4) and (3,5)!"
</pre>

<p>The walls set uses the coarse-grained (8x8) mapping for rows and columns, which should work out well for your approximator.</p>
For further details, refer to <a href="http://www.cs.utexas.edu/users/risto/cs343/private/hw-last-year/hw3/hw3_faq.html">FAQs page</a>.
<h4>Switching between approaches</h4>
All three approaches (pure tabular, tiling, and nearest neighbors) should be implemented such that they can be easily switched between (e.g., by commenting out a piece of python code or passing a different parameter) so that it is easy to check that each works.

<h3>Debugging</h3>
If you run into any bugs in your program, you can find the error log file for OpenNERO at one of the following locations:

<ul>
<li><b>Linux or Mac:</b> <tt>~/.opennero/nero_log.txt</tt></li>
<li><b>Windows:</b> <tt>"AppData\Local\OpenNERO\nero_log.txt"</tt> or <tt>"Local Settings\Application Data\OpenNERO\nero_log.txt"</tt> depending on the version of Windows you have.</li>
</ul>

<H2>Turning in your work</H2>
Turn in your report and all the files that you have modified electronically using <A HREF="http://www.cs.utexas.edu/users/risto/cs343/turnin.html"><TT>turnin</TT></A> (grader: <tt>houck</tt>, homework: <tt>cs343-hw2</tt>). In your report, include any instructions needed to run your agent, as well as a brief description of how your agent implements tiling, and nearest neighbors interpolation. Be sure to include <b>exact instructions</b> about how to switch between tabular, tiling, and nearest neighbors, as all three methods will be evaluated. As mentioned before, it may be easiest to just "tar" the directory and submit the tar-file to <TT>turnin</TT>.
 
</body> 
</html> 
