Readings: due Tuesday 2/25 (email response due Mon 2/24 by 8 pm)
Textbook Chapter 21.1-21.3
[optional] Sections 6.1, 6.2, 6.5 in Sutton and Barto
Email response: write a solution for Exercise 17.10 (a-b); 21.2
============================================================================

17.10 Consider an undiscounted MDP having three states, (1, 2, 3), with
rewards −1, −2, 0, respectively. State 3 is a terminal state. In states 1 and
2 there are two possible actions: a and b. The transition model is as follows:
• In state 1, action a moves the agent to state 2 with probability 0.8 and
makes the agent stay put with probability 0.2.
• In state 2, action a moves the agent to state 1 with probability 0.8 and
makes the agent stay put with probability 0.2.
• In either state 1 or state 2, action b moves the agent to state 3 with
probability 0.1 and makes the agent stay put with probability 0.9.

Answer the following questions:
a. What can be determined qualitatively about the optimal policy in states 1
and 2?

    Optimal Action in state 1 and 2. 

b. Apply policy iteration, showing each step in full, to determine the optimal
policy and the values of states 1 and 2. Assume that the initial policy has
action b in both states.
    
    FIRST ITERATION
    S    S1  S2 
    pi_1  b   b
    POLICY EVALUTION:
    V_1(s_1) = -0.9   
    V_1(s_2) = -1.8  
    .......
    V_n(s_1) = -0.25
    V_n(s_2) = -18

    POLICY IMPROVEMENT:
    pi_2(s_1) = argmax(-16.25, -1) = b
    pi_2(s_2) = argmax(-5, -16) = a
    S    S1  S2 
    pi_2  b   a 

    SECOND ITERATION
    V_0(s_1) = 0
    V_0(s_2) = 0
    .......
    V_n(s_1) = -9
    V_n(s_2) = -7.5 

    policy improvement:
    pi_3(s_1) = argmax(-9.6, -9) = b
    pi_3(s_2) = argmax(-7.5, -8.55) = a

    Since the policy does not vary any more, the optimal policy for state 1 is
    action b, and the optimal policy for state 2 is action a.

============================================================================
21.2 Chapter 17 defined a proper policy for an MDP as one that is guaranteed
to reach a terminal state. 

Show that it is possible for a passive ADP agent to learn a transition model
for which its policy π is improper even if π is proper for the true MDP; 

Passive learning agent does not know the transition model P (s′ | s, a), which
specifies the probability of reaching state s′ from state s after doing action
a; nor does it know the reward function R(s), which specifies the reward for
each state. The agent executes a set of trials in the environment using its
policy π. Since it evaluates all possible policy, thus the passive learning
agent would definitely find an optimal policy.

With such models, the POLICY-EVALUATION step may fail if γ=1. Show that this
problem cannot arise if POLICY-EVALUATION is applied to the learned model only
at the end of a trial.

The object is to use the information about rewards to learn the expected
utility Uπ(s) associated with each nonterminal state s. The utility is
defined to be the expected sum of (discounted) rewards obtained if policy π is
followed. The model may fail since the trial may not reach the terminal state,
which means it continues to experiment one trial in non-terminal state.

============================================================================
Readings: due Thursday 2/27 (email response due Wed 2/26 by 8 pm)
Textbook Chapter 21.4-21.6
Email response: write a reading response following the syllabus guidelines.  


Previously, it is introduced two specific form of reinforcement learning, the
passive and adaptive reinforcement learning. However, the utility function and
Q-functions learned by the agent is not scalable to the large space since that
would be inefficient. Thus, in 21.4, approaches, such like function
approximation, are introduced to increase the generalizability to large state
space. 

In 21.5, another approach of reinforcement learning, the policy search method.
The basic idea of policy search method is keep twiddling the policy as long as
its performance improves, then stop. A policy π is a function that maps states
to actions. We are interested primarily in parameterized representations of π
that have far fewer parameters than there are states in the state space.
Policy search will then adjust the parameters θ to improve the policy.
For
    π(s) = max Qˆθ(s, a) 
Notice that if the policy is represented by Q- functions, then policy search
results in a process that learns Q-functions. This process is not the same as
Q-learning.

For 21.6, a series of applications of reinforcement learning in the area of
game playing and robotics controlling are introduced. In the area of game
playing, there are checker program written by Arthur Samuel and Gerry Tesauro’s
backgammon program TD-GAMMON. In the area of robot control, there are inverted
pendulum problem where the state variables are continuous.

