%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  THIS TEX FILE IS TO GENERATE PDF FILE FOR 
%%% 
%%%  COPYRIGHT (C) JIMMY LIN, 2013, UT AUSTIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,a4paper]{report}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  PACKAGES USED IN THIS TEX SOURCE FILE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry,amsthm,amsmath,graphicx,amssymb,fancyheadings}
\usepackage[]{mcode}
\usepackage[colorlinks,
            linkcolor=blue,
            anchorcolor=red,
            citecolor=green
            ]{hyperref}
% for my mac
\IfFileExists{/Users/JimmyLin/.latex/UTA_CS/JS.sty}{ 
    \usepackage{/Users/JimmyLin/.latex/UTA_CS/JS}
    \usepackage{/Users/JimmyLin/.latex/UTA_CS/JSASGN}
}{} 
% for UT's linux machine
\IfFileExists{/u/jimmylin/workspace/Configs/latex/UTA_CS/JS.sty}{
    \usepackage{/u/jimmylin/.latex/UTA_CS/JS} 
    \usepackage{/u/jimmylin/.latex/UTA_CS/JSASGN}
}{} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% MACROS CONTAINING THE FILE INFORMATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\COURSE}{EE381V Large Scale Optimization}
\renewcommand{\LECTURER}{Sujay Sanghavi}
\renewcommand{\SECTION}{17350}
\renewcommand{\TASK}{Problem Set 0}
\renewcommand{\RELEASEDATE}{September 5, 2014}
\renewcommand{\DUEDATE}{September 11, 2014}
\renewcommand{\TIMECONSUME}{10 hours}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% DOCUMENTATION STARTS FROM HERE 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE PAGE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
    \maketitle
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CONTENT PAGE: TABLEOFCONTENTS, LISTOFTABLES, LIST OF FIGURES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\contentsname}{Table of Contents}
\begin{center} 
    \tableofcontents 
    %\listoftables 
    %\listoffigures
\end{center}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% GENERAL DOCUMENTATION BEGINS 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Matlab and Computational Assignment}
\section{Algorithm 1: Least Square}
The command to invoke standarded least-squared regression:
\begin{verbatim}
>> algo1()
\end{verbatim}
Note that {\it algo1.m} includes scripts for all three datasets.
%{{{
\subsection{Small-scale dataset: Succeed}
The brief summary of applying standarded least-squared regression on
small-scale dataset is as follows:
\begin{itemize}
    \item  Total CPU time (secs)  = $0.18$  

    \item   CPU time per iteration = $0.02$

    \item   Regression Error $||X \beta - y||$: 1.1698e-10
    
    \item Testing Error $||X_{test} \beta - y_{test}||$: $23.058394$ (pretty large)
\end{itemize}

\subsection{Medium-scale dataset: Succeed}
The brief summary of applying standarded least-squared regression on
medium-scale dataset is as follows:
\begin{itemize}
 \item   Total CPU time (secs)  = $43.95$  

 \item   CPU time per iteration = $5.49$ 

 \item   Regression Error $||X \beta - y||$: 3.2594e-09
    
  \item  Testing Error $||X_{test} \beta - y_{test}||$: $19.862394$ (pretty large)
\end{itemize}
\subsection{Large-scale dataset: Failed}
    
    This standarded least-square regression task is too large-scaled to be computed. 
%}}}

\newpage
\section{Algorithm 2: optimization with LASSO}
The command to invoke least-squared regression with LASSO:
\begin{verbatim}
>> algo2()
\end{verbatim}
Note that {\it algo2.m} includes scripts for all three datasets.
%{{{
\subsection{Small-scale dataset: Succeed}
The brief summary of applying least-squared regression with LASSO on
small-scale dataset is as follows:
\begin{itemize}
 \item   Total CPU time (secs)  = $0.38$ 

 \item   CPU time per iteration = $0.02$  

 \item   Regression Error: 6.7886e-10

 \item   Testing Error: $0.144338$

 \item   Supports (non-zeros entries of $\beta$): $43$ ($500$ atoms in total)
\end{itemize}

\subsection{Medium-scale dataset: Succeed}
The brief summary of applying least-squared regression with LASSO on
medium-scale dataset is as follows:
\begin{itemize}
 \item   Total CPU time (secs)  = $126.66$

 \item   CPU time per iteration = $4.87$  

 \item   Regression Error: 4.4292e-09
 
 \item   Testing Error: $0.078289$

 \item   Supports (non-zeros entries of $\beta$): $342$ ($5000$ atoms in total)
\end{itemize}

\subsection{Large-scale dataset: Failed}
    
    This least-square regression with LASSO task is too large-scaled to be computed. 
%}}}
\\[0.3cm]

\noindent 
{\bf Remarks}: Least-squared regression
with LASSO does outperfrom standarded least-squared regression in its
prediction accuracy. Besides, it has higher computational complexity since it
requires more iterations for convergence and each iteration cost more time to
complete.

\newpage
\section{Orthogonal Matching Pursuit}
The command to invoke regression with OMP preprocessing:
\begin{verbatim}
>> regress_omp()
\end{verbatim}

\subsection{Small-scale Dataset: Succeed}
The brief summary of applying regression with OMP feature selection on
small-scale dataset is as follows:
\begin{itemize}
\item Indices of Features selected by OMP (with order): 402, 235, 86, 11, 108. 
\item Elapsed time is 0.198106 seconds.
\item Regression Error $||X \beta - y||$: 5.3785e-02
\item Testing Error $||X_{test} \beta - y_{test}||$: 4.4208e-02
\end{itemize}

\subsection{Medium-scale Dataset: Succeed}
The brief summary of applying regression with OMP feature selection on
medium-scale dataset is as follows:
\begin{itemize}
\item Indices of Features selected by OMP (with order): 577, 2760, 561, 3614,
    3958. 
\item Elapsed time is 0.209093 seconds.
\item Regression Error $||X \beta - y||$: 2.1955e-01
\item Testing Error $||X_{test} \beta - y_{test}||$: 1.8219e-02
\end{itemize}

\subsection{Large-scale Dataset: Succeed}
The brief summary of applying regression with OMP feature selection on
large-scale dataset is as follows:
\begin{itemize}
\item Indices of Features selected by OMP (with order): 17099, 29426, 35373,
    22452, 43354. 
\item Elapsed time is 2.994790 seconds.
\item Regression Error $||X \beta - y||$: 6.9964e-01
\item Testing Error $||X_{test} \beta - y_{test}||$: 6.4437e-03
\end{itemize}

Note that Elapsed time is defined as OMP preprocessing and regression for
selected atoms on that dataset, but not included computation for regression
error and testing error. 
\\[0.3cm]

\noindent 
{\bf Remarks}: Least-squared regression on OMP feature selection performs much
better than standarded least-squared regression and least-squared regression
with LASSO. Besides, it has lower computational complexity since it allows the
large-scale dataset (third dataset) to be regressed.

\chapter{Linear Algebra Review}
\section{More Range and Nullspace}
\subsection{Smallest and Largest rank of $C = AB$}

{\bf Conditions}: 
$A \in \mathbb{R}^{10 \times 10}$ with $rank(A) = 5$ and $B \in \mathbb{R}^{10 \times 10}$ with
$rank(B) = 5$. 

Sylvester's rank inequality: 
$\forall A \in R^{m\times k}, B \in \mathbb{R}^{k\times n}$
$$rank(A) + rank(B) - k \leq rank(AB)$$

Smallest rank of $C = AB$ is $rank(A) + rank(B) - k = 5 + 5 - 10 = 0$.

Largest rank of $C = AB$ is $min(rank(A), rank(B)) = min(5,5) = 5$.

\subsection{Largest rank of $C = AB$}
{\bf Conditions}: 
$A \in \mathbb{R}^{10 \times 15}$ with $rank(A) = 7$ and $B \in \mathbb{R}^{15 \times 11}$ with
$rank(B) = 8$. 

Largest rank of $C = AB$ is $min(rank(A), rank(B)) = min(7,8) = 7$.

\section{Riesz Representation Theorem}
\newcommand{\w}{\mathbf{w}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\e}[1]{\mathbf{e}_{#1}}
\newcommand{\alp}[1]{\mathbf{\alpha}_{#1}}
\newcommand{\linearcombo}{\alp{1} \e{1} + \alp{2} \e{2} + \dots + \alp{n} \e{n}}
Linear map $f: \mathbb{R}^n \rightarrow \mathbb{R}$ has two critical properties due to its linearity: 
\begin{align}
    \text{additivity: }& f(x + y) = f(x) + f(y), \forall x, y \in dom(f)
    \label{LM:additivity} \\
    \text{homogeneity: }& f(\alpha x) = \alpha f(x), \forall \alpha \in
    \mathbb{R}, x \in dom(f) \label{LM:homogeneity}
\end{align}
Let arbitrary vector $\w = (\alp{1}, \alp{2}, \dots, \alp{n}) \in \mathbb{R}^{n}$.
Then we can denote $\w$ as linear combination of standard basis
\begin{align}
    \w = \linearcombo \label{RRT:linearCombo}
\end{align}
Now we start to show that $f(\w)$ can be represented as inner product of $\w$
and another vector.
\begin{align}
    f(\w) &= f(\linearcombo) 
    && \text{standard basis representation} \eqref{RRT:linearCombo} \\
    &= f(\alp{1} \e{1}) + f(\alp{2} \e{2}) + \dots + f(\alp{n} \e{n}) 
    && \text{additivity of linear map} \eqref{LM:additivity} \\
    &= \alp{1} f(\e{1}) + \alp{2} f(\e{2}) + \dots + \alp{n} f(\e{n})
    && \text{additivity of linear map} \eqref{LM:homogeneity} \\
    &= \langle(f(\e{1}), f(\e{2}), \dots, f(\e{n})), (\alp{1}, \alp{2}, \dots, \alp{n}) \rangle 
    && \text{definition of inner product} \\
    &= \langle \x, \w \rangle 
    && \x = (f(\e{1}), f(\e{2}), \dots, f(\e{n}))
\end{align}
Hence, we have successfully proved that 
\begin{align}
    \forall \text{ linear map } f:\mathbb{R}^n \rightarrow \mathbb{R}, \exists \x \in \mathbb{R}^n, f(\w) = \langle \x, \w \rangle
\end{align}

\newpage
\section{Polynomial Vector Spaces}
\subsection{$Tp=2p(t)-tp'(t)$: True}
$T$ is represented by an upper bi-diagonal matrix with all diagonal entries
being $a_0=2$ and sub-diagonal entries to be $a_1=-1$. Obviously, the matrix
is full-rank and hence the range is the entire $(d+1)$-dimensional space.
\subsection{$Tp=2p(t)-3tp'(t)$: True}
$T$ is represented by an upper bi-diagonal matrix with all diagonal entries
being $a_0=2$ and sub-diagonal entries to be $a_1=-3$.
Obviously, the matrix is full-rank and hence the range is the entire
$(d+1)$-dimensional space.
\subsection{Characterization of Surjectivity: $a_0 \not = 0$}

If $a_0 = 0$, the matrix representing $T$ is not full-rank anymore. The
dimensionality of range is not $(d+1)$, which suggests that the vector $q$ with
highest degree $d$ is not reachable by abitrary pair of $Tp$. This also
indicates that if $a_0 = 0$,  the corresponding mapping $T$ is not surjective
anymore: for every polynomial(vector) $q \in V$, there does not exist a
polynomial(vector) $p \in V$ such that $Tp = q$. In conclusion, we have
\begin{align}
    a_0 \not = 0 \implies \forall q \in V, \exists p \in V, s.t.\ Tp=q
\end{align}

\newcommand{\rank}[1]{rank(#1)}
\section{Rank}
\subsection{Show that $\rank{A} \leq min\{m, n\}$}
\begin{proof}
\rank{A} is defined as the number of columns that are linearly
independent. Then, we have 
\begin{align}  \label{RANK:1-col}
    \rank{A} \leq \text{number of linearly independent columns} \leq n
\end{align}
Since the number of linearly independent columns equals to the
number of linearly independent rows, 
\begin{align} \label{RANK:1-row}
    \rank{A} \leq \text{number of linearly independent rows} \leq m
\end{align}
From \eqref{RANK:1-col} and \eqref{RANK:1-row}, it is easy to derive the
desired result:
\begin{align} \label{RANK:1-result}
    \rank{A} \leq  min\{m, n\}
\end{align}
\end{proof}
\subsection{Sylvester's rank inequality}
\begin{proof}
    adf    
\end{proof}

\subsection{Subadditivity}

\subsection{Frobenius Rank Inequality}

\newpage
\appendix
\chapter{Codes Printout}

\section{Sparse Recovery}
\subsection{Algorithm 1: Least Square}
\lstinputlisting{../ps0_matlab/algo1.m}
\newpage
\subsection{Algorithm 2: Optimization with LASSO}
\lstinputlisting{../ps0_matlab/algo2.m}
\newpage

\section{Orthogonal Matching Pursuit}

\subsection{OMP Routine}
\lstinputlisting{../ps0_matlab/omp.m}

\newpage
\subsection{Regression Scripts}
\lstinputlisting{../ps0_matlab/regress_omp.m}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% General Documentation ends
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
