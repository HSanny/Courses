%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  THIS TEX FILE IS TO GENERATE PDF FILE FOR 
%%% 
%%%  COPYRIGHT (C) JIMMY LIN, 2013, UT AUSTIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,a4paper]{report}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  PACKAGES USED IN THIS TEX SOURCE FILE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry,amsthm,amsmath,graphicx,fancyheadings}
\usepackage[]{mcode}
\usepackage[colorlinks,
            linkcolor=blue,
            anchorcolor=red,
            citecolor=green
            ]{hyperref}
% for my mac
\IfFileExists{/Users/JimmyLin/.latex/UTA_CS/JS.sty}{ 
    \usepackage{/Users/JimmyLin/.latex/UTA_CS/JS}
    \usepackage{/Users/JimmyLin/.latex/UTA_CS/JSASGN}
}{} 
% for UT's linux machine
\IfFileExists{/u/jimmylin/workspace/Configs/latex/UTA_CS/JS.sty}{
    \usepackage{/u/jimmylin/.latex/UTA_CS/JS} 
    \usepackage{/u/jimmylin/.latex/UTA_CS/JSASGN}
}{} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% MACROS CONTAINING THE FILE INFORMATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\COURSE}{EE381V Large Scale Optimization}
\renewcommand{\LECTURER}{Sujay Sanghavi}
\renewcommand{\SECTION}{17350}
\renewcommand{\TASK}{Problem Set 0}
\renewcommand{\RELEASEDATE}{September 5, 2014}
\renewcommand{\DUEDATE}{September 11, 2014}
\renewcommand{\TIMECONSUME}{10 hours}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% DOCUMENTATION STARTS FROM HERE 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE PAGE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
    \maketitle
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CONTENT PAGE: TABLEOFCONTENTS, LISTOFTABLES, LIST OF FIGURES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\contentsname}{Table of Contents}
\begin{center} 
    \tableofcontents 
    %\listoftables 
    %\listoffigures
\end{center}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% GENERAL DOCUMENTATION BEGINS 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Matlab and Computational Assignment}
\section{Algorithm 1: Least Square}
The command to invoke standarded least-squared regression:
\begin{verbatim}
>> algo1()
\end{verbatim}
Note that {\it algo1.m} includes scripts for all three datasets.
%{{{
\subsection{Small-scale dataset: Succeed}
The brief summary of applying standarded least-squared regression on
small-scale dataset is as follows:
\begin{itemize}
    \item  Total CPU time (secs)  = $0.18$  

    \item   CPU time per iteration = $0.02$

    \item   Regression Error $||X \beta - y||$: 1.1698e-10
    
    \item Testing Error $||X_{test} \beta - y_{test}||$: $23.058394$ (pretty large)
\end{itemize}

\subsection{Medium-scale dataset: Succeed}
The brief summary of applying standarded least-squared regression on
medium-scale dataset is as follows:
\begin{itemize}
 \item   Total CPU time (secs)  = $43.95$  

 \item   CPU time per iteration = $5.49$ 

 \item   Regression Error $||X \beta - y||$: 3.2594e-09
    
  \item  Testing Error $||X_{test} \beta - y_{test}||$: $19.862394$ (pretty large)
\end{itemize}
\subsection{Large-scale dataset: Failed}
    
    This standarded least-square regression task is too large-scaled to be computed. 
%}}}

\newpage
\section{Algorithm 2: optimization with LASSO}
The command to invoke least-squared regression with LASSO:
\begin{verbatim}
>> algo2()
\end{verbatim}
Note that {\it algo2.m} includes scripts for all three datasets.
%{{{
\subsection{Small-scale dataset: Succeed}
The brief summary of applying least-squared regression with LASSO on
small-scale dataset is as follows:
\begin{itemize}
 \item   Total CPU time (secs)  = $0.38$ 

 \item   CPU time per iteration = $0.02$  

 \item   Regression Error: 6.7886e-10

 \item   Testing Error: $0.144338$

 \item   Supports (non-zeros entries of $\beta$): $43$ ($500$ atoms in total)
\end{itemize}

\subsection{Medium-scale dataset: Succeed}
The brief summary of applying least-squared regression with LASSO on
medium-scale dataset is as follows:
\begin{itemize}
 \item   Total CPU time (secs)  = $126.66$

 \item   CPU time per iteration = $4.87$  

 \item   Regression Error: 4.4292e-09
 
 \item   Testing Error: $0.078289$

 \item   Supports (non-zeros entries of $\beta$): $342$ ($5000$ atoms in total)
\end{itemize}

\subsection{Large-scale dataset: Failed}
    
    This least-square regression with LASSO task is too large-scaled to be computed. 
%}}}
\\[0.3cm]

\noindent 
{\bf Remarks}: Least-squared regression
with LASSO does outperfrom standarded least-squared regression in its
prediction accuracy. Besides, it has higher computational complexity since it
requires more iterations for convergence and each iteration cost more time to
complete.

\newpage
\section{Orthogonal Matching Pursuit}
The command to invoke regression with OMP preprocessing:
\begin{verbatim}
>> regress_omp()
\end{verbatim}

\subsection{Small-scale Dataset: Succeed}
The brief summary of applying regression with OMP feature selection on
small-scale dataset is as follows:
\begin{itemize}
\item Indices of Features selected by OMP (with order): 402, 235, 86, 11, 108. 
\item Elapsed time is 0.198106 seconds.
\item Regression Error $||X \beta - y||$: 5.3785e-02
\item Testing Error $||X_{test} \beta - y_{test}||$: 4.4208e-02
\end{itemize}

\subsection{Medium-scale Dataset: Succeed}
The brief summary of applying regression with OMP feature selection on
medium-scale dataset is as follows:
\begin{itemize}
\item Indices of Features selected by OMP (with order): 577, 2760, 561, 3614,
    3958. 
\item Elapsed time is 0.209093 seconds.
\item Regression Error $||X \beta - y||$: 2.1955e-01
\item Testing Error $||X_{test} \beta - y_{test}||$: 1.8219e-02
\end{itemize}

\subsection{Large-scale Dataset: Succeed}
The brief summary of applying regression with OMP feature selection on
large-scale dataset is as follows:
\begin{itemize}
\item Indices of Features selected by OMP (with order): 17099, 29426, 35373,
    22452, 43354. 
\item Elapsed time is 2.994790 seconds.
\item Regression Error $||X \beta - y||$: 6.9964e-01
\item Testing Error $||X_{test} \beta - y_{test}||$: 6.4437e-03
\end{itemize}

Note that Elapsed time is defined as OMP preprocessing and regression for
selected atoms on that dataset, but not included computation for regression
error and testing error. 
\\[0.3cm]

\noindent 
{\bf Remarks}: Least-squared regression on OMP feature selection performs much
better than standarded least-squared regression and least-squared regression
with LASSO. Besides, it has lower computational complexity since it allows the
large-scale dataset (third dataset) to be regressed.

%\chapter{Linear Algebra Review}
%\section{Vector Spaces}
%
%\section{Linear Operators}
%
%\section{Independence}
%
%\section{Linearly Independence}
%
%\section{Range and Nullspace of Matrices}
%
%\section{More Range and Nullspace}
%
%\section{Riesz Representation Theorem}
%
%\section{Derivatives}
%
%\section{Rank}
%
%\section{Surjectivity}

\newpage
\appendix
\chapter{Codes Printout}

\section{Sparse Recovery}
\subsection{Algorithm 1: Least Square}
\lstinputlisting{../ps0_matlab/algo1.m}
\newpage
\subsection{Algorithm 2: Optimization with LASSO}
\lstinputlisting{../ps0_matlab/algo2.m}
\newpage

\section{Orthogonal Matching Pursuit}

\subsection{OMP Routine}
\lstinputlisting{../ps0_matlab/omp.m}

\newpage
\subsection{Regression Scripts}
\lstinputlisting{../ps0_matlab/regress_omp.m}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% General Documentation ends
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
