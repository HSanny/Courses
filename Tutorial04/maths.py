#!/usr/bin/python2.7
''' 
    Filename:   maths.py    
    Version:    1.0
    Since:      2013-03-28
    Author: 
        Jimmy Lin (u5223173) - u5223173@uds.anu.edu.au
        
    Edited by MacVim
    Documentation auto-generated by Snippet 
'''
import math
import numpy as np

def xor(x1, x2):
    return x1 != x2

def tanh(a):
    return (math.e**a - math.e**(-a)) / ( math.e**a + math.e**(-a))

def sigma(a):
    return 1.0 / ( 1 + math.e**(-a) )
    
class Unit():
    def __init__(self, label, acFunc = "tanh"):
        self.label = label
        self.value = 0
        self.acFunc = acFunc
        return

    def getValue(self):
        return self.value

    def setValue(self, val):
        self.value = val
        return

    def activate(self, a):
        if self.acFunc == "tanh":
            self.value = tanh(a)
        elif self.acFunc == "sigma":
            self.value = sigma(a)
        return 

class InputUnit(Unit):
    def __init__(self, label):
        Unit.__init__(self,label)
        return 

class HiddenUnit(Unit):
    def __init__(self, label):
        Unit.__init__(self,label)
        return 

class OutputUnit(Unit):
    def __init__(self, label):
        Unit.__init__(self,label)
        return 


class Layer():
    def __init__(self, label):
        self.label = label
        self.unitList = []
        return

    def getValueVector(self):
        valueVector = []
        for i in range(0, len(self.unitList)):
            valueVector.append(self.unitList[i].getValue())
        return valueVector

    def addUnit(self, u):
        self.unitList.append(u)
        return

    def getSize(self):
        return len(self.unitList)

    def getUnit(self, index):
        assert type(self.unitList) == type([])
        return self.unitList[index]

class InputLayer(Layer):
    def __init__(self, label):
        Layer.__init__(self,label)
        return


class HiddenLayer(Layer):
    def __init__(self, label, bias = True):
        Layer.__init__(self, label)
        return 

class OutputLayer(Layer):
    def __init__(self, label):
        Layer.__init__(self,label)
        return

class Network():
    def __init__(self, label, inp = None, hid = [ ], out = None):
        assert isinstance(hid, list)
        self.label = label

        self.inputL = inp
        self.hiddenL = hid
        self.outputL = out

        self.weightMatrix = None
        self.biasOpen = True
        return 

    def setInputLayer(self, inp):
        if isinstance(inp, InputLayer):
            self.inputL = inp

    def addHiddenLayer(self, hid):
        if isinstance(hid, HiddenLayer):
            self.hiddenL.append(hid)
        return 

    def setOutputLayer(self, out):
        if isinstance(out, OutputLayer):
            self.outputL = out
    
    def getSize(self):
        assert self.inputL is not None
        assert isinstance(self.hiddenL, list)
        assert self.outputL is not None
        sizeVector =  []
        sizeVector.append(self.inputL.getSize())
        for i in range(0, len(self.hiddenL)):
            sizeVector.append(self.hiddenL[i].getSize())
        sizeVector.append(self.outputL.getSize())
        return sizeVector

    def initRandomMatrix(self):
        sizeVector = self.getSize()
        depth = len(sizeVector) - 1
        randomMatrix = []
        for i in range(0, depth):
            if self.biasOpen:
                tempMatrix = np.random.rand(sizeVector[i]+1, sizeVector[i+1])
            else:
                tempMatrix = np.random.rand(sizeVector[i], sizeVector[i+1])
            randomMatrix.append(tempMatrix)
        self.weightMatrix = randomMatrix

        return True

    def forwardFeed(self, inputVector):
        ## store inputVector into input node
        assert self.inputL.getSize() == len(inputVector)
        for i in range(0, len(inputVector)):
            self.inputL.getUnit(i).setValue(inputVector[i])
        ## start to feed
        if self.biasOpen:
            offset = 1
        else:
            offset = 0
        ## feed from input Layer to first hidden Layer
        for i in range(0, self.hiddenL[0].getSize()):
            temp = 0
            if self.biasOpen:
                temp += 1 * self.weightMatrix[0][0][i]
            for j in range(0, self.inputL.getSize()):
                temp += self.inputL.getUnit(j).getValue() * self.weightMatrix[0][j+offset][i]
            self.hiddenL[0].getUnit(i).activate(temp)

        ## feed between hidden Layers
        for k in range(0, len(self.hiddenL) - 1):
            for i in range(0, len(self.hiddenL[k+1])):
                temp = 0
                if self.biasOpen:
                    temp += 1 * self.weightMatrix[k][0][i]
                for j in range(0, len(self.hiddenL[k])):
                    temp += self.hiddenL[k].getUnit(j).getValue() * self.weightMatrix[k+1][j+offset][i]
                self.hiddenL[k+1].getUnit(i).activate(temp)

        ## feed from last hidden Layers to the output Layer
        for i in range(0, self.outputL.getSize()):
            temp = 0
            if self.biasOpen:
                temp += 1 * self.weightMatrix[0][0][i]
            for j in range(0, self.inputL.getSize()):
                temp += self.inputL.getUnit(j).getValue() * self.weightMatrix[-1][j+offset][i]
            self.outputL.getUnit(i).activate(temp)

        return

    def backPropagate(self):

        return

    def run(self):
        self.weightMatrix = self.createRandomMatrix()
        
        self.forwardFeed()
        self.backPropagate()

        return 

    def testInitMatrix(self):
        assert self.initRandomMatrix() 
        print "initRandomMatrix for breaking symmetry"
        for each in self.weightMatrix:
            print each

    def testForwardFeed(self, inputData):
        self.forwardFeed(inputData)
        print "feed forward test:"
        ilstr = "1. input Layer values:\n"
        for each in self.inputL.unitList:
             ilstr += str(each.getValue()) + " "
        print ilstr

        hlstr = "2. hidden Layer value:"
        for l in range(0, len(self.hiddenL)):
            hlstr += '\n'
            for each in self.hiddenL[l].unitList:
                hlstr += str(each.getValue()) + " "
        print hlstr 

        flstr = "3. final Layer value:\n"
        for each in self.outputL.unitList:
            flstr += str(each.getValue())+ " "
        print flstr


def createNetwork():
    ## Input Layer
    IU1 = InputUnit("InputUnit-1")
    IU2 = InputUnit("InputUnit-2")
    IL = InputLayer("InputLayer")
    IL.addUnit(IU1)
    IL.addUnit(IU2)

    ## Hidden Layer
    HU1 = HiddenUnit("HiddenUnit-1")
    HU2 = HiddenUnit("HiddenUnit-2")
    HU3 = HiddenUnit("HiddenUnit-3")
    HL = HiddenLayer("HiddenLayer-1")
    HL.addUnit(HU1)
    HL.addUnit(HU2)
    HL.addUnit(HU3)

    ## Output Layer
    OU1 = OutputUnit("OutputUnit-1")
    OL = OutputLayer("OutputLayer")
    OL.addUnit(OU1)
    
    ## Establish Neural Network
    neuralNet = Network("Neural Network simulating XOR function") 
    neuralNet.setInputLayer(IL)
    neuralNet.addHiddenLayer(HL)
    neuralNet.setOutputLayer(OL)

    return neuralNet
    

def testMathFunction():
    ## Test for xor(x1,x2)
    assert not xor(1,1)
    assert xor(0,1)
    assert xor(1,0)
    assert not xor(0,0)

    ## Test for tanh(a)
    print tanh(2)
    print tanh(-2)
    print tanh(0)

    ## Test for sigma(a)
    print sigma(2)
    print sigma(-2)
    print sigma(0)

if __name__ == "__main__":
    neuralNet = createNetwork()
    neuralNet.testInitMatrix()
    neuralNet.testForwardFeed([1,0])

